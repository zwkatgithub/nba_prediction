{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "\n",
    "'''通过方差的百分比来计算将数据降到多少维是比较合适的，\n",
    "函数传入的参数是特征值和百分比percentage，返回需要降到的维度数num'''\n",
    "def eigValPct(eigVals,percentage):\n",
    "    sortArray=sort(eigVals) #使用numpy中的sort()对特征值按照从小到大排序\n",
    "    sortArray=sortArray[-1::-1] #特征值从大到小排序\n",
    "    arraySum=sum(sortArray) #数据全部的方差arraySum\n",
    "    tempSum=0\n",
    "    num=0\n",
    "    for i in sortArray:\n",
    "        tempSum+=i\n",
    "        num+=1\n",
    "        if tempSum>=arraySum*percentage:\n",
    "            return num\n",
    "\n",
    "'''pca函数有两个参数，其中dataMat是已经转换成矩阵matrix形式的数据集，列表示特征；\n",
    "其中的percentage表示取前多少个特征需要达到的方差占比，默认为0.9'''\n",
    "def pca(dataMat,percentage=0.9):\n",
    "    meanVals=mean(dataMat,axis=0)  #对每一列求平均值，因为协方差的计算中需要减去均值\n",
    "    meanRemoved=dataMat-meanVals\n",
    "    covMat=cov(meanRemoved,rowvar=0)  #cov()计算方差\n",
    "    eigVals,eigVects=linalg.eig(mat(covMat))  #利用numpy中寻找特征值和特征向量的模块linalg中的eig()方法\n",
    "    k=eigValPct(eigVals,percentage) #要达到方差的百分比percentage，需要前k个向量\n",
    "    eigValInd=argsort(eigVals)  #对特征值eigVals从小到大排序\n",
    "    eigValInd=eigValInd[:-(k+1):-1] #从排好序的特征值，从后往前取k个，这样就实现了特征值的从大到小排列\n",
    "    redEigVects=eigVects[:,eigValInd]   #返回排序后特征值对应的特征向量redEigVects（主成分）\n",
    "    lowDDataMat=meanRemoved*redEigVects #将原始数据投影到主成分上得到新的低维数据lowDDataMat\n",
    "    reconMat=(lowDDataMat*redEigVects.T)+meanVals   #得到重构数据reconMat\n",
    "    return lowDDataMat,reconMat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/processed/data.csv','r') as f:\n",
    "    rawdata = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = rawdata.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawdata = [[float(d) for d in r.split(',')] for r in a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rawdata = np.array(rawdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24118, 64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawdata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = rawdata[:,:55]\n",
    "label = rawdata[:,55:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24118, 55)\n"
     ]
    }
   ],
   "source": [
    "n_data = normalize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a,b = pca(n_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24118, 34)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trainers import MLPTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mxnet import gluon, nd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainer = MLPTrainer('three_pt',gluon.loss.SoftmaxCrossEntropyLoss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainer.dataload(nd.array(train_data),train_label, nd.array(test_data),test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from write_data import OutputData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = label[:,OutputData.colName().index('three_pt')]*3+label[:,OutputData.colName().index('ft')] + label[:,OutputData.colName().index('in_pts')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r = nd.cast(nd.array(res > 0),'float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.initnet(0.00001,0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 : Train loss -> 0.873270 Test loss -> 0.861508 Train acc -> 0.459957 Test acc -> 0.473051\n",
      "Epoch 1 : Train loss -> 0.864892 Test loss -> 0.853730 Train acc -> 0.462919 Test acc -> 0.476368\n",
      "Epoch 2 : Train loss -> 0.856844 Test loss -> 0.846259 Train acc -> 0.467006 Test acc -> 0.480238\n",
      "Epoch 3 : Train loss -> 0.849057 Test loss -> 0.839089 Train acc -> 0.470027 Test acc -> 0.483554\n",
      "Epoch 4 : Train loss -> 0.841470 Test loss -> 0.832029 Train acc -> 0.472634 Test acc -> 0.485627\n",
      "Epoch 5 : Train loss -> 0.834279 Test loss -> 0.825353 Train acc -> 0.476188 Test acc -> 0.487839\n",
      "Epoch 6 : Train loss -> 0.827290 Test loss -> 0.818854 Train acc -> 0.479090 Test acc -> 0.490326\n",
      "Epoch 7 : Train loss -> 0.820615 Test loss -> 0.812635 Train acc -> 0.482585 Test acc -> 0.492676\n",
      "Epoch 8 : Train loss -> 0.814126 Test loss -> 0.806597 Train acc -> 0.486258 Test acc -> 0.494472\n",
      "Epoch 9 : Train loss -> 0.807896 Test loss -> 0.800769 Train acc -> 0.490167 Test acc -> 0.497651\n",
      "Epoch 10 : Train loss -> 0.801901 Test loss -> 0.795153 Train acc -> 0.493425 Test acc -> 0.501382\n",
      "Epoch 11 : Train loss -> 0.796106 Test loss -> 0.789697 Train acc -> 0.496564 Test acc -> 0.503593\n",
      "Epoch 12 : Train loss -> 0.790485 Test loss -> 0.784449 Train acc -> 0.499585 Test acc -> 0.506495\n",
      "Epoch 13 : Train loss -> 0.785058 Test loss -> 0.779322 Train acc -> 0.502725 Test acc -> 0.510779\n",
      "Epoch 14 : Train loss -> 0.779860 Test loss -> 0.774423 Train acc -> 0.506753 Test acc -> 0.513405\n",
      "Epoch 15 : Train loss -> 0.774798 Test loss -> 0.769643 Train acc -> 0.510070 Test acc -> 0.515202\n",
      "Epoch 16 : Train loss -> 0.769889 Test loss -> 0.765046 Train acc -> 0.513565 Test acc -> 0.518519\n",
      "Epoch 17 : Train loss -> 0.765215 Test loss -> 0.760663 Train acc -> 0.516586 Test acc -> 0.520868\n",
      "Epoch 18 : Train loss -> 0.760669 Test loss -> 0.756328 Train acc -> 0.518955 Test acc -> 0.524876\n",
      "Epoch 19 : Train loss -> 0.756269 Test loss -> 0.752185 Train acc -> 0.522450 Test acc -> 0.527363\n",
      "Epoch 20 : Train loss -> 0.752043 Test loss -> 0.748204 Train acc -> 0.524819 Test acc -> 0.530680\n",
      "Epoch 21 : Train loss -> 0.747973 Test loss -> 0.744376 Train acc -> 0.528788 Test acc -> 0.535102\n",
      "Epoch 22 : Train loss -> 0.743989 Test loss -> 0.740565 Train acc -> 0.532105 Test acc -> 0.537590\n",
      "Epoch 23 : Train loss -> 0.740144 Test loss -> 0.736945 Train acc -> 0.533408 Test acc -> 0.539801\n",
      "Epoch 24 : Train loss -> 0.736424 Test loss -> 0.733442 Train acc -> 0.536489 Test acc -> 0.543256\n",
      "Epoch 25 : Train loss -> 0.732811 Test loss -> 0.730058 Train acc -> 0.539806 Test acc -> 0.546573\n",
      "Epoch 26 : Train loss -> 0.729326 Test loss -> 0.726757 Train acc -> 0.542886 Test acc -> 0.548507\n",
      "Epoch 27 : Train loss -> 0.725959 Test loss -> 0.723573 Train acc -> 0.546381 Test acc -> 0.552792\n",
      "Epoch 28 : Train loss -> 0.722658 Test loss -> 0.720478 Train acc -> 0.548750 Test acc -> 0.555417\n",
      "Epoch 29 : Train loss -> 0.719487 Test loss -> 0.717467 Train acc -> 0.551771 Test acc -> 0.557490\n",
      "Epoch 30 : Train loss -> 0.716423 Test loss -> 0.714570 Train acc -> 0.555266 Test acc -> 0.561913\n",
      "Epoch 31 : Train loss -> 0.713429 Test loss -> 0.711763 Train acc -> 0.558465 Test acc -> 0.563018\n",
      "Epoch 32 : Train loss -> 0.710583 Test loss -> 0.709052 Train acc -> 0.561130 Test acc -> 0.566473\n",
      "Epoch 33 : Train loss -> 0.707808 Test loss -> 0.706440 Train acc -> 0.564210 Test acc -> 0.571310\n",
      "Epoch 34 : Train loss -> 0.705118 Test loss -> 0.703892 Train acc -> 0.566165 Test acc -> 0.574212\n",
      "Epoch 35 : Train loss -> 0.702509 Test loss -> 0.701410 Train acc -> 0.568831 Test acc -> 0.576838\n",
      "Epoch 36 : Train loss -> 0.699966 Test loss -> 0.698997 Train acc -> 0.571496 Test acc -> 0.579878\n",
      "Epoch 37 : Train loss -> 0.697522 Test loss -> 0.696682 Train acc -> 0.574458 Test acc -> 0.581813\n",
      "Epoch 38 : Train loss -> 0.695156 Test loss -> 0.694448 Train acc -> 0.577124 Test acc -> 0.583886\n",
      "Epoch 39 : Train loss -> 0.692856 Test loss -> 0.692269 Train acc -> 0.579078 Test acc -> 0.586788\n",
      "Epoch 40 : Train loss -> 0.690617 Test loss -> 0.690169 Train acc -> 0.581507 Test acc -> 0.589138\n",
      "Epoch 41 : Train loss -> 0.688474 Test loss -> 0.688178 Train acc -> 0.583639 Test acc -> 0.591625\n",
      "Epoch 42 : Train loss -> 0.686386 Test loss -> 0.686186 Train acc -> 0.586009 Test acc -> 0.593145\n",
      "Epoch 43 : Train loss -> 0.684353 Test loss -> 0.684262 Train acc -> 0.587134 Test acc -> 0.595771\n",
      "Epoch 44 : Train loss -> 0.682379 Test loss -> 0.682405 Train acc -> 0.589089 Test acc -> 0.598259\n",
      "Epoch 45 : Train loss -> 0.680488 Test loss -> 0.680616 Train acc -> 0.590333 Test acc -> 0.600055\n",
      "Epoch 46 : Train loss -> 0.678640 Test loss -> 0.678877 Train acc -> 0.592051 Test acc -> 0.603234\n",
      "Epoch 47 : Train loss -> 0.676857 Test loss -> 0.677239 Train acc -> 0.593709 Test acc -> 0.605307\n",
      "Epoch 48 : Train loss -> 0.675134 Test loss -> 0.675587 Train acc -> 0.595368 Test acc -> 0.605860\n",
      "Epoch 49 : Train loss -> 0.673427 Test loss -> 0.674014 Train acc -> 0.597678 Test acc -> 0.606965\n",
      "Epoch 50 : Train loss -> 0.671793 Test loss -> 0.672498 Train acc -> 0.599396 Test acc -> 0.609315\n",
      "Epoch 51 : Train loss -> 0.670198 Test loss -> 0.670996 Train acc -> 0.601410 Test acc -> 0.610835\n",
      "Epoch 52 : Train loss -> 0.668679 Test loss -> 0.669578 Train acc -> 0.603779 Test acc -> 0.610835\n",
      "Epoch 53 : Train loss -> 0.667184 Test loss -> 0.668150 Train acc -> 0.605912 Test acc -> 0.612631\n",
      "Epoch 54 : Train loss -> 0.665732 Test loss -> 0.666824 Train acc -> 0.607096 Test acc -> 0.614013\n",
      "Epoch 55 : Train loss -> 0.664303 Test loss -> 0.665454 Train acc -> 0.609406 Test acc -> 0.616224\n",
      "Epoch 56 : Train loss -> 0.662952 Test loss -> 0.664214 Train acc -> 0.610769 Test acc -> 0.618297\n",
      "Epoch 57 : Train loss -> 0.661599 Test loss -> 0.662949 Train acc -> 0.613198 Test acc -> 0.618988\n",
      "Epoch 58 : Train loss -> 0.660311 Test loss -> 0.661750 Train acc -> 0.614738 Test acc -> 0.620094\n",
      "Epoch 59 : Train loss -> 0.659049 Test loss -> 0.660578 Train acc -> 0.616929 Test acc -> 0.622443\n",
      "Epoch 60 : Train loss -> 0.657824 Test loss -> 0.659437 Train acc -> 0.618410 Test acc -> 0.624240\n",
      "Epoch 61 : Train loss -> 0.656652 Test loss -> 0.658349 Train acc -> 0.620069 Test acc -> 0.626313\n",
      "Epoch 62 : Train loss -> 0.655491 Test loss -> 0.657269 Train acc -> 0.622083 Test acc -> 0.626451\n",
      "Epoch 63 : Train loss -> 0.654389 Test loss -> 0.656236 Train acc -> 0.623327 Test acc -> 0.627833\n",
      "Epoch 64 : Train loss -> 0.653285 Test loss -> 0.655203 Train acc -> 0.624689 Test acc -> 0.629491\n",
      "Epoch 65 : Train loss -> 0.652220 Test loss -> 0.654228 Train acc -> 0.625696 Test acc -> 0.630459\n",
      "Epoch 66 : Train loss -> 0.651183 Test loss -> 0.653255 Train acc -> 0.627118 Test acc -> 0.630597\n",
      "Epoch 67 : Train loss -> 0.650166 Test loss -> 0.652329 Train acc -> 0.628776 Test acc -> 0.630044\n",
      "Epoch 68 : Train loss -> 0.649200 Test loss -> 0.651421 Train acc -> 0.629724 Test acc -> 0.630459\n",
      "Epoch 69 : Train loss -> 0.648234 Test loss -> 0.650560 Train acc -> 0.630790 Test acc -> 0.631150\n",
      "Epoch 70 : Train loss -> 0.647317 Test loss -> 0.649696 Train acc -> 0.631916 Test acc -> 0.632117\n",
      "Epoch 71 : Train loss -> 0.646411 Test loss -> 0.648871 Train acc -> 0.633397 Test acc -> 0.632255\n",
      "Epoch 72 : Train loss -> 0.645511 Test loss -> 0.648033 Train acc -> 0.634167 Test acc -> 0.632946\n",
      "Epoch 73 : Train loss -> 0.644659 Test loss -> 0.647256 Train acc -> 0.635529 Test acc -> 0.634052\n",
      "Epoch 74 : Train loss -> 0.643823 Test loss -> 0.646494 Train acc -> 0.636121 Test acc -> 0.634743\n",
      "Epoch 75 : Train loss -> 0.643028 Test loss -> 0.645780 Train acc -> 0.636654 Test acc -> 0.635158\n",
      "Epoch 76 : Train loss -> 0.642248 Test loss -> 0.645043 Train acc -> 0.637069 Test acc -> 0.635572\n",
      "Epoch 77 : Train loss -> 0.641473 Test loss -> 0.644333 Train acc -> 0.638313 Test acc -> 0.636401\n",
      "Epoch 78 : Train loss -> 0.640723 Test loss -> 0.643649 Train acc -> 0.639142 Test acc -> 0.637645\n",
      "Epoch 79 : Train loss -> 0.639993 Test loss -> 0.642976 Train acc -> 0.640090 Test acc -> 0.637783\n",
      "Epoch 80 : Train loss -> 0.639286 Test loss -> 0.642322 Train acc -> 0.641216 Test acc -> 0.638751\n",
      "Epoch 81 : Train loss -> 0.638593 Test loss -> 0.641691 Train acc -> 0.641749 Test acc -> 0.639442\n",
      "Epoch 82 : Train loss -> 0.637917 Test loss -> 0.641075 Train acc -> 0.642696 Test acc -> 0.639856\n",
      "Epoch 83 : Train loss -> 0.637254 Test loss -> 0.640475 Train acc -> 0.643585 Test acc -> 0.639994\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84 : Train loss -> 0.636612 Test loss -> 0.639899 Train acc -> 0.644710 Test acc -> 0.640409\n",
      "Epoch 85 : Train loss -> 0.635974 Test loss -> 0.639297 Train acc -> 0.644592 Test acc -> 0.641100\n",
      "Epoch 86 : Train loss -> 0.635370 Test loss -> 0.638769 Train acc -> 0.645303 Test acc -> 0.641238\n",
      "Epoch 87 : Train loss -> 0.634755 Test loss -> 0.638217 Train acc -> 0.646547 Test acc -> 0.641376\n",
      "Epoch 88 : Train loss -> 0.634174 Test loss -> 0.637673 Train acc -> 0.647494 Test acc -> 0.641791\n",
      "Epoch 89 : Train loss -> 0.633592 Test loss -> 0.637170 Train acc -> 0.648027 Test acc -> 0.642758\n",
      "Epoch 90 : Train loss -> 0.633042 Test loss -> 0.636693 Train acc -> 0.648561 Test acc -> 0.643311\n",
      "Epoch 91 : Train loss -> 0.632489 Test loss -> 0.636188 Train acc -> 0.649212 Test acc -> 0.643588\n",
      "Epoch 92 : Train loss -> 0.631955 Test loss -> 0.635708 Train acc -> 0.649627 Test acc -> 0.644140\n",
      "Epoch 93 : Train loss -> 0.631434 Test loss -> 0.635242 Train acc -> 0.650338 Test acc -> 0.644831\n",
      "Epoch 94 : Train loss -> 0.630926 Test loss -> 0.634788 Train acc -> 0.650989 Test acc -> 0.645937\n",
      "Epoch 95 : Train loss -> 0.630419 Test loss -> 0.634358 Train acc -> 0.651108 Test acc -> 0.645661\n",
      "Epoch 96 : Train loss -> 0.629932 Test loss -> 0.633912 Train acc -> 0.651937 Test acc -> 0.646904\n",
      "Epoch 97 : Train loss -> 0.629452 Test loss -> 0.633473 Train acc -> 0.652352 Test acc -> 0.646766\n",
      "Epoch 98 : Train loss -> 0.628975 Test loss -> 0.633050 Train acc -> 0.653240 Test acc -> 0.647595\n",
      "Epoch 99 : Train loss -> 0.628519 Test loss -> 0.632650 Train acc -> 0.653418 Test acc -> 0.648563\n",
      "Epoch 100 : Train loss -> 0.628059 Test loss -> 0.632261 Train acc -> 0.653536 Test acc -> 0.648563\n",
      "Epoch 101 : Train loss -> 0.627610 Test loss -> 0.631854 Train acc -> 0.654129 Test acc -> 0.648977\n",
      "Epoch 102 : Train loss -> 0.627184 Test loss -> 0.631496 Train acc -> 0.654188 Test acc -> 0.649945\n",
      "Epoch 103 : Train loss -> 0.626761 Test loss -> 0.631126 Train acc -> 0.654129 Test acc -> 0.650774\n",
      "Epoch 104 : Train loss -> 0.626345 Test loss -> 0.630765 Train acc -> 0.654484 Test acc -> 0.651050\n",
      "Epoch 105 : Train loss -> 0.625935 Test loss -> 0.630403 Train acc -> 0.654721 Test acc -> 0.652294\n",
      "Epoch 106 : Train loss -> 0.625525 Test loss -> 0.630052 Train acc -> 0.654839 Test acc -> 0.652018\n",
      "Epoch 107 : Train loss -> 0.625137 Test loss -> 0.629700 Train acc -> 0.655076 Test acc -> 0.651879\n",
      "Epoch 108 : Train loss -> 0.624741 Test loss -> 0.629371 Train acc -> 0.655787 Test acc -> 0.652294\n",
      "Epoch 109 : Train loss -> 0.624356 Test loss -> 0.629066 Train acc -> 0.656617 Test acc -> 0.652294\n",
      "Epoch 110 : Train loss -> 0.623976 Test loss -> 0.628738 Train acc -> 0.657090 Test acc -> 0.651603\n",
      "Epoch 111 : Train loss -> 0.623603 Test loss -> 0.628394 Train acc -> 0.657387 Test acc -> 0.652018\n",
      "Epoch 112 : Train loss -> 0.623244 Test loss -> 0.628098 Train acc -> 0.657742 Test acc -> 0.651465\n",
      "Epoch 113 : Train loss -> 0.622890 Test loss -> 0.627817 Train acc -> 0.658097 Test acc -> 0.651465\n",
      "Epoch 114 : Train loss -> 0.622538 Test loss -> 0.627517 Train acc -> 0.658216 Test acc -> 0.651327\n",
      "Epoch 115 : Train loss -> 0.622195 Test loss -> 0.627218 Train acc -> 0.658334 Test acc -> 0.652156\n",
      "Epoch 116 : Train loss -> 0.621862 Test loss -> 0.626916 Train acc -> 0.658630 Test acc -> 0.652570\n",
      "Epoch 117 : Train loss -> 0.621523 Test loss -> 0.626644 Train acc -> 0.658986 Test acc -> 0.652432\n",
      "Epoch 118 : Train loss -> 0.621202 Test loss -> 0.626359 Train acc -> 0.659223 Test acc -> 0.652156\n",
      "Epoch 119 : Train loss -> 0.620876 Test loss -> 0.626081 Train acc -> 0.659638 Test acc -> 0.652570\n",
      "Epoch 120 : Train loss -> 0.620559 Test loss -> 0.625809 Train acc -> 0.659815 Test acc -> 0.652847\n",
      "Epoch 121 : Train loss -> 0.620250 Test loss -> 0.625544 Train acc -> 0.660052 Test acc -> 0.652985\n",
      "Epoch 122 : Train loss -> 0.619939 Test loss -> 0.625279 Train acc -> 0.660171 Test acc -> 0.652847\n",
      "Epoch 123 : Train loss -> 0.619646 Test loss -> 0.625035 Train acc -> 0.660881 Test acc -> 0.653123\n",
      "Epoch 124 : Train loss -> 0.619343 Test loss -> 0.624794 Train acc -> 0.661237 Test acc -> 0.653123\n",
      "Epoch 125 : Train loss -> 0.619056 Test loss -> 0.624569 Train acc -> 0.660763 Test acc -> 0.653261\n",
      "Epoch 126 : Train loss -> 0.618766 Test loss -> 0.624338 Train acc -> 0.661059 Test acc -> 0.653400\n",
      "Epoch 127 : Train loss -> 0.618482 Test loss -> 0.624076 Train acc -> 0.661474 Test acc -> 0.653400\n",
      "Epoch 128 : Train loss -> 0.618209 Test loss -> 0.623856 Train acc -> 0.661651 Test acc -> 0.653676\n",
      "Epoch 129 : Train loss -> 0.617938 Test loss -> 0.623623 Train acc -> 0.662185 Test acc -> 0.653952\n",
      "Epoch 130 : Train loss -> 0.617670 Test loss -> 0.623411 Train acc -> 0.662481 Test acc -> 0.654505\n",
      "Epoch 131 : Train loss -> 0.617397 Test loss -> 0.623176 Train acc -> 0.663251 Test acc -> 0.654367\n",
      "Epoch 132 : Train loss -> 0.617136 Test loss -> 0.622973 Train acc -> 0.663428 Test acc -> 0.655058\n",
      "Epoch 133 : Train loss -> 0.616882 Test loss -> 0.622732 Train acc -> 0.663843 Test acc -> 0.655473\n",
      "Epoch 134 : Train loss -> 0.616629 Test loss -> 0.622526 Train acc -> 0.663962 Test acc -> 0.655611\n",
      "Epoch 135 : Train loss -> 0.616378 Test loss -> 0.622322 Train acc -> 0.664139 Test acc -> 0.655334\n",
      "Epoch 136 : Train loss -> 0.616131 Test loss -> 0.622129 Train acc -> 0.664554 Test acc -> 0.655749\n",
      "Epoch 137 : Train loss -> 0.615888 Test loss -> 0.621920 Train acc -> 0.664791 Test acc -> 0.656440\n",
      "Epoch 138 : Train loss -> 0.615651 Test loss -> 0.621735 Train acc -> 0.665087 Test acc -> 0.656440\n",
      "Epoch 139 : Train loss -> 0.615410 Test loss -> 0.621540 Train acc -> 0.665206 Test acc -> 0.656578\n",
      "Epoch 140 : Train loss -> 0.615179 Test loss -> 0.621367 Train acc -> 0.665620 Test acc -> 0.656993\n",
      "Epoch 141 : Train loss -> 0.614944 Test loss -> 0.621154 Train acc -> 0.665502 Test acc -> 0.656716\n",
      "Epoch 142 : Train loss -> 0.614718 Test loss -> 0.620964 Train acc -> 0.665620 Test acc -> 0.656855\n",
      "Epoch 143 : Train loss -> 0.614493 Test loss -> 0.620770 Train acc -> 0.666153 Test acc -> 0.656993\n",
      "Epoch 144 : Train loss -> 0.614265 Test loss -> 0.620617 Train acc -> 0.666390 Test acc -> 0.657407\n",
      "Epoch 145 : Train loss -> 0.614050 Test loss -> 0.620423 Train acc -> 0.666331 Test acc -> 0.657684\n",
      "Epoch 146 : Train loss -> 0.613828 Test loss -> 0.620249 Train acc -> 0.666509 Test acc -> 0.657822\n",
      "Epoch 147 : Train loss -> 0.613612 Test loss -> 0.620054 Train acc -> 0.666568 Test acc -> 0.658237\n",
      "Epoch 148 : Train loss -> 0.613402 Test loss -> 0.619900 Train acc -> 0.666746 Test acc -> 0.658098\n",
      "Epoch 149 : Train loss -> 0.613192 Test loss -> 0.619734 Train acc -> 0.667042 Test acc -> 0.658237\n",
      "Epoch 150 : Train loss -> 0.612985 Test loss -> 0.619544 Train acc -> 0.667042 Test acc -> 0.658098\n",
      "Epoch 151 : Train loss -> 0.612779 Test loss -> 0.619379 Train acc -> 0.667516 Test acc -> 0.658237\n",
      "Epoch 152 : Train loss -> 0.612580 Test loss -> 0.619215 Train acc -> 0.667456 Test acc -> 0.658513\n",
      "Epoch 153 : Train loss -> 0.612380 Test loss -> 0.619037 Train acc -> 0.667397 Test acc -> 0.659066\n",
      "Epoch 154 : Train loss -> 0.612183 Test loss -> 0.618891 Train acc -> 0.667634 Test acc -> 0.659066\n",
      "Epoch 155 : Train loss -> 0.611986 Test loss -> 0.618716 Train acc -> 0.668227 Test acc -> 0.659342\n",
      "Epoch 156 : Train loss -> 0.611789 Test loss -> 0.618562 Train acc -> 0.668641 Test acc -> 0.659480\n",
      "Epoch 157 : Train loss -> 0.611603 Test loss -> 0.618418 Train acc -> 0.668997 Test acc -> 0.659757\n",
      "Epoch 158 : Train loss -> 0.611408 Test loss -> 0.618263 Train acc -> 0.669115 Test acc -> 0.660310\n",
      "Epoch 159 : Train loss -> 0.611216 Test loss -> 0.618100 Train acc -> 0.669411 Test acc -> 0.659619\n",
      "Epoch 160 : Train loss -> 0.611033 Test loss -> 0.617943 Train acc -> 0.669115 Test acc -> 0.659757\n",
      "Epoch 161 : Train loss -> 0.610848 Test loss -> 0.617799 Train acc -> 0.669411 Test acc -> 0.659757\n",
      "Epoch 162 : Train loss -> 0.610664 Test loss -> 0.617675 Train acc -> 0.669470 Test acc -> 0.659619\n",
      "Epoch 163 : Train loss -> 0.610485 Test loss -> 0.617528 Train acc -> 0.669470 Test acc -> 0.659342\n",
      "Epoch 164 : Train loss -> 0.610301 Test loss -> 0.617380 Train acc -> 0.669530 Test acc -> 0.659619\n",
      "Epoch 165 : Train loss -> 0.610125 Test loss -> 0.617212 Train acc -> 0.669293 Test acc -> 0.659757\n",
      "Epoch 166 : Train loss -> 0.609946 Test loss -> 0.617098 Train acc -> 0.669885 Test acc -> 0.660033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 167 : Train loss -> 0.609776 Test loss -> 0.616958 Train acc -> 0.670004 Test acc -> 0.660171\n",
      "Epoch 168 : Train loss -> 0.609600 Test loss -> 0.616814 Train acc -> 0.670004 Test acc -> 0.660586\n",
      "Epoch 169 : Train loss -> 0.609433 Test loss -> 0.616692 Train acc -> 0.670477 Test acc -> 0.660724\n",
      "Epoch 170 : Train loss -> 0.609263 Test loss -> 0.616549 Train acc -> 0.670477 Test acc -> 0.660862\n",
      "Epoch 171 : Train loss -> 0.609095 Test loss -> 0.616428 Train acc -> 0.670714 Test acc -> 0.661001\n",
      "Epoch 172 : Train loss -> 0.608931 Test loss -> 0.616297 Train acc -> 0.670714 Test acc -> 0.661001\n",
      "Epoch 173 : Train loss -> 0.608769 Test loss -> 0.616153 Train acc -> 0.670892 Test acc -> 0.661001\n",
      "Epoch 174 : Train loss -> 0.608607 Test loss -> 0.616034 Train acc -> 0.671011 Test acc -> 0.661277\n",
      "Epoch 175 : Train loss -> 0.608444 Test loss -> 0.615920 Train acc -> 0.671129 Test acc -> 0.661553\n",
      "Epoch 176 : Train loss -> 0.608285 Test loss -> 0.615789 Train acc -> 0.670951 Test acc -> 0.661553\n",
      "Epoch 177 : Train loss -> 0.608127 Test loss -> 0.615670 Train acc -> 0.670774 Test acc -> 0.661553\n",
      "Epoch 178 : Train loss -> 0.607974 Test loss -> 0.615559 Train acc -> 0.671011 Test acc -> 0.662521\n",
      "Epoch 179 : Train loss -> 0.607817 Test loss -> 0.615437 Train acc -> 0.670951 Test acc -> 0.663488\n",
      "Epoch 180 : Train loss -> 0.607669 Test loss -> 0.615328 Train acc -> 0.670951 Test acc -> 0.663350\n",
      "Epoch 181 : Train loss -> 0.607515 Test loss -> 0.615188 Train acc -> 0.670951 Test acc -> 0.663765\n",
      "Epoch 182 : Train loss -> 0.607363 Test loss -> 0.615100 Train acc -> 0.671307 Test acc -> 0.663903\n",
      "Epoch 183 : Train loss -> 0.607217 Test loss -> 0.614967 Train acc -> 0.671721 Test acc -> 0.663903\n",
      "Epoch 184 : Train loss -> 0.607068 Test loss -> 0.614852 Train acc -> 0.671781 Test acc -> 0.663765\n",
      "Epoch 185 : Train loss -> 0.606922 Test loss -> 0.614745 Train acc -> 0.671840 Test acc -> 0.663903\n",
      "Epoch 186 : Train loss -> 0.606779 Test loss -> 0.614641 Train acc -> 0.671899 Test acc -> 0.664041\n",
      "Epoch 187 : Train loss -> 0.606635 Test loss -> 0.614544 Train acc -> 0.671899 Test acc -> 0.663626\n",
      "Epoch 188 : Train loss -> 0.606498 Test loss -> 0.614411 Train acc -> 0.672018 Test acc -> 0.663350\n",
      "Epoch 189 : Train loss -> 0.606358 Test loss -> 0.614320 Train acc -> 0.672136 Test acc -> 0.662797\n",
      "Epoch 190 : Train loss -> 0.606219 Test loss -> 0.614189 Train acc -> 0.672195 Test acc -> 0.662383\n",
      "Epoch 191 : Train loss -> 0.606079 Test loss -> 0.614078 Train acc -> 0.672018 Test acc -> 0.662797\n",
      "Epoch 192 : Train loss -> 0.605942 Test loss -> 0.613983 Train acc -> 0.672254 Test acc -> 0.662797\n",
      "Epoch 193 : Train loss -> 0.605807 Test loss -> 0.613874 Train acc -> 0.672610 Test acc -> 0.662935\n",
      "Epoch 194 : Train loss -> 0.605675 Test loss -> 0.613775 Train acc -> 0.672728 Test acc -> 0.662935\n",
      "Epoch 195 : Train loss -> 0.605540 Test loss -> 0.613671 Train acc -> 0.672847 Test acc -> 0.663074\n",
      "Epoch 196 : Train loss -> 0.605407 Test loss -> 0.613562 Train acc -> 0.673025 Test acc -> 0.663626\n",
      "Epoch 197 : Train loss -> 0.605276 Test loss -> 0.613477 Train acc -> 0.673143 Test acc -> 0.663626\n",
      "Epoch 198 : Train loss -> 0.605145 Test loss -> 0.613369 Train acc -> 0.673439 Test acc -> 0.664041\n",
      "Epoch 199 : Train loss -> 0.605015 Test loss -> 0.613283 Train acc -> 0.673617 Test acc -> 0.664317\n",
      "Epoch 200 : Train loss -> 0.604884 Test loss -> 0.613184 Train acc -> 0.673735 Test acc -> 0.664317\n",
      "Epoch 201 : Train loss -> 0.604757 Test loss -> 0.613098 Train acc -> 0.674150 Test acc -> 0.664179\n",
      "Epoch 202 : Train loss -> 0.604630 Test loss -> 0.613016 Train acc -> 0.674505 Test acc -> 0.664179\n",
      "Epoch 203 : Train loss -> 0.604505 Test loss -> 0.612936 Train acc -> 0.674505 Test acc -> 0.664179\n",
      "Epoch 204 : Train loss -> 0.604379 Test loss -> 0.612859 Train acc -> 0.674683 Test acc -> 0.664594\n",
      "Epoch 205 : Train loss -> 0.604256 Test loss -> 0.612762 Train acc -> 0.674920 Test acc -> 0.664732\n",
      "Epoch 206 : Train loss -> 0.604131 Test loss -> 0.612670 Train acc -> 0.675039 Test acc -> 0.665285\n",
      "Epoch 207 : Train loss -> 0.604007 Test loss -> 0.612578 Train acc -> 0.675039 Test acc -> 0.665285\n",
      "Epoch 208 : Train loss -> 0.603886 Test loss -> 0.612503 Train acc -> 0.675216 Test acc -> 0.665008\n",
      "Epoch 209 : Train loss -> 0.603767 Test loss -> 0.612397 Train acc -> 0.675216 Test acc -> 0.665976\n",
      "Epoch 210 : Train loss -> 0.603648 Test loss -> 0.612317 Train acc -> 0.675098 Test acc -> 0.666114\n",
      "Epoch 211 : Train loss -> 0.603530 Test loss -> 0.612229 Train acc -> 0.675216 Test acc -> 0.666114\n",
      "Epoch 212 : Train loss -> 0.603417 Test loss -> 0.612154 Train acc -> 0.675275 Test acc -> 0.665837\n",
      "Epoch 213 : Train loss -> 0.603297 Test loss -> 0.612078 Train acc -> 0.675631 Test acc -> 0.666114\n",
      "Epoch 214 : Train loss -> 0.603182 Test loss -> 0.611984 Train acc -> 0.675631 Test acc -> 0.666252\n",
      "Epoch 215 : Train loss -> 0.603067 Test loss -> 0.611907 Train acc -> 0.675512 Test acc -> 0.666252\n",
      "Epoch 216 : Train loss -> 0.602955 Test loss -> 0.611824 Train acc -> 0.675749 Test acc -> 0.666114\n",
      "Epoch 217 : Train loss -> 0.602842 Test loss -> 0.611752 Train acc -> 0.675749 Test acc -> 0.666528\n",
      "Epoch 218 : Train loss -> 0.602730 Test loss -> 0.611693 Train acc -> 0.676223 Test acc -> 0.666667\n",
      "Epoch 219 : Train loss -> 0.602620 Test loss -> 0.611609 Train acc -> 0.676342 Test acc -> 0.666943\n",
      "Epoch 220 : Train loss -> 0.602511 Test loss -> 0.611529 Train acc -> 0.676460 Test acc -> 0.667219\n",
      "Epoch 221 : Train loss -> 0.602402 Test loss -> 0.611454 Train acc -> 0.676638 Test acc -> 0.667081\n",
      "Epoch 222 : Train loss -> 0.602291 Test loss -> 0.611388 Train acc -> 0.676756 Test acc -> 0.667358\n",
      "Epoch 223 : Train loss -> 0.602183 Test loss -> 0.611314 Train acc -> 0.676756 Test acc -> 0.667219\n",
      "Epoch 224 : Train loss -> 0.602076 Test loss -> 0.611243 Train acc -> 0.676875 Test acc -> 0.667634\n",
      "Epoch 225 : Train loss -> 0.601969 Test loss -> 0.611184 Train acc -> 0.677289 Test acc -> 0.667634\n",
      "Epoch 226 : Train loss -> 0.601862 Test loss -> 0.611119 Train acc -> 0.677467 Test acc -> 0.667496\n",
      "Epoch 227 : Train loss -> 0.601754 Test loss -> 0.611044 Train acc -> 0.677763 Test acc -> 0.667081\n",
      "Epoch 228 : Train loss -> 0.601650 Test loss -> 0.610974 Train acc -> 0.677941 Test acc -> 0.667358\n",
      "Epoch 229 : Train loss -> 0.601541 Test loss -> 0.610897 Train acc -> 0.677941 Test acc -> 0.667358\n",
      "Epoch 230 : Train loss -> 0.601440 Test loss -> 0.610837 Train acc -> 0.678119 Test acc -> 0.667081\n",
      "Epoch 231 : Train loss -> 0.601336 Test loss -> 0.610759 Train acc -> 0.678119 Test acc -> 0.666805\n",
      "Epoch 232 : Train loss -> 0.601234 Test loss -> 0.610677 Train acc -> 0.678059 Test acc -> 0.667081\n",
      "Epoch 233 : Train loss -> 0.601131 Test loss -> 0.610618 Train acc -> 0.678000 Test acc -> 0.667081\n",
      "Epoch 234 : Train loss -> 0.601031 Test loss -> 0.610558 Train acc -> 0.678178 Test acc -> 0.667219\n",
      "Epoch 235 : Train loss -> 0.600932 Test loss -> 0.610511 Train acc -> 0.678356 Test acc -> 0.667358\n",
      "Epoch 236 : Train loss -> 0.600831 Test loss -> 0.610436 Train acc -> 0.678356 Test acc -> 0.666943\n",
      "Epoch 237 : Train loss -> 0.600735 Test loss -> 0.610381 Train acc -> 0.678178 Test acc -> 0.666943\n",
      "Epoch 238 : Train loss -> 0.600637 Test loss -> 0.610307 Train acc -> 0.678178 Test acc -> 0.666805\n",
      "Epoch 239 : Train loss -> 0.600541 Test loss -> 0.610237 Train acc -> 0.677882 Test acc -> 0.666805\n",
      "Epoch 240 : Train loss -> 0.600444 Test loss -> 0.610172 Train acc -> 0.678059 Test acc -> 0.667219\n",
      "Epoch 241 : Train loss -> 0.600348 Test loss -> 0.610104 Train acc -> 0.677763 Test acc -> 0.667358\n",
      "Epoch 242 : Train loss -> 0.600257 Test loss -> 0.610045 Train acc -> 0.677763 Test acc -> 0.667358\n",
      "Epoch 243 : Train loss -> 0.600161 Test loss -> 0.609971 Train acc -> 0.677763 Test acc -> 0.667219\n",
      "Epoch 244 : Train loss -> 0.600071 Test loss -> 0.609928 Train acc -> 0.677823 Test acc -> 0.666943\n",
      "Epoch 245 : Train loss -> 0.599979 Test loss -> 0.609866 Train acc -> 0.677882 Test acc -> 0.666805\n",
      "Epoch 246 : Train loss -> 0.599888 Test loss -> 0.609823 Train acc -> 0.677882 Test acc -> 0.667081\n",
      "Epoch 247 : Train loss -> 0.599798 Test loss -> 0.609759 Train acc -> 0.678000 Test acc -> 0.667219\n",
      "Epoch 248 : Train loss -> 0.599710 Test loss -> 0.609688 Train acc -> 0.678178 Test acc -> 0.667634\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 249 : Train loss -> 0.599619 Test loss -> 0.609638 Train acc -> 0.678296 Test acc -> 0.667496\n",
      "Epoch 250 : Train loss -> 0.599530 Test loss -> 0.609584 Train acc -> 0.678474 Test acc -> 0.667634\n",
      "Epoch 251 : Train loss -> 0.599442 Test loss -> 0.609526 Train acc -> 0.678356 Test acc -> 0.667496\n",
      "Epoch 252 : Train loss -> 0.599355 Test loss -> 0.609458 Train acc -> 0.678474 Test acc -> 0.667496\n",
      "Epoch 253 : Train loss -> 0.599267 Test loss -> 0.609411 Train acc -> 0.678652 Test acc -> 0.667634\n",
      "Epoch 254 : Train loss -> 0.599181 Test loss -> 0.609343 Train acc -> 0.678593 Test acc -> 0.667634\n",
      "Epoch 255 : Train loss -> 0.599094 Test loss -> 0.609297 Train acc -> 0.678415 Test acc -> 0.667910\n",
      "Epoch 256 : Train loss -> 0.599009 Test loss -> 0.609236 Train acc -> 0.678415 Test acc -> 0.668187\n",
      "Epoch 257 : Train loss -> 0.598923 Test loss -> 0.609196 Train acc -> 0.678533 Test acc -> 0.668187\n",
      "Epoch 258 : Train loss -> 0.598838 Test loss -> 0.609134 Train acc -> 0.678711 Test acc -> 0.668325\n",
      "Epoch 259 : Train loss -> 0.598752 Test loss -> 0.609078 Train acc -> 0.678711 Test acc -> 0.668049\n",
      "Epoch 260 : Train loss -> 0.598669 Test loss -> 0.609028 Train acc -> 0.678830 Test acc -> 0.668187\n",
      "Epoch 261 : Train loss -> 0.598587 Test loss -> 0.608952 Train acc -> 0.679066 Test acc -> 0.668463\n",
      "Epoch 262 : Train loss -> 0.598503 Test loss -> 0.608894 Train acc -> 0.679126 Test acc -> 0.668601\n",
      "Epoch 263 : Train loss -> 0.598420 Test loss -> 0.608841 Train acc -> 0.679185 Test acc -> 0.668740\n",
      "Epoch 264 : Train loss -> 0.598339 Test loss -> 0.608797 Train acc -> 0.679244 Test acc -> 0.668740\n",
      "Epoch 265 : Train loss -> 0.598259 Test loss -> 0.608759 Train acc -> 0.679303 Test acc -> 0.668740\n",
      "Epoch 266 : Train loss -> 0.598181 Test loss -> 0.608710 Train acc -> 0.679126 Test acc -> 0.668740\n",
      "Epoch 267 : Train loss -> 0.598100 Test loss -> 0.608674 Train acc -> 0.679126 Test acc -> 0.669016\n",
      "Epoch 268 : Train loss -> 0.598022 Test loss -> 0.608624 Train acc -> 0.679066 Test acc -> 0.669016\n",
      "Epoch 269 : Train loss -> 0.597943 Test loss -> 0.608569 Train acc -> 0.679303 Test acc -> 0.669431\n",
      "Epoch 270 : Train loss -> 0.597864 Test loss -> 0.608527 Train acc -> 0.679303 Test acc -> 0.669292\n",
      "Epoch 271 : Train loss -> 0.597788 Test loss -> 0.608451 Train acc -> 0.679481 Test acc -> 0.669016\n",
      "Epoch 272 : Train loss -> 0.597711 Test loss -> 0.608422 Train acc -> 0.679303 Test acc -> 0.669569\n",
      "Epoch 273 : Train loss -> 0.597635 Test loss -> 0.608362 Train acc -> 0.679481 Test acc -> 0.669431\n",
      "Epoch 274 : Train loss -> 0.597560 Test loss -> 0.608305 Train acc -> 0.679244 Test acc -> 0.669292\n",
      "Epoch 275 : Train loss -> 0.597484 Test loss -> 0.608252 Train acc -> 0.679185 Test acc -> 0.668878\n",
      "Epoch 276 : Train loss -> 0.597411 Test loss -> 0.608201 Train acc -> 0.679244 Test acc -> 0.668878\n",
      "Epoch 277 : Train loss -> 0.597336 Test loss -> 0.608185 Train acc -> 0.679659 Test acc -> 0.669154\n",
      "Epoch 278 : Train loss -> 0.597260 Test loss -> 0.608131 Train acc -> 0.679600 Test acc -> 0.668878\n",
      "Epoch 279 : Train loss -> 0.597187 Test loss -> 0.608070 Train acc -> 0.679718 Test acc -> 0.669016\n",
      "Epoch 280 : Train loss -> 0.597114 Test loss -> 0.608022 Train acc -> 0.679659 Test acc -> 0.669292\n",
      "Epoch 281 : Train loss -> 0.597041 Test loss -> 0.607967 Train acc -> 0.679955 Test acc -> 0.668878\n",
      "Epoch 282 : Train loss -> 0.596968 Test loss -> 0.607925 Train acc -> 0.680073 Test acc -> 0.669292\n",
      "Epoch 283 : Train loss -> 0.596898 Test loss -> 0.607899 Train acc -> 0.680073 Test acc -> 0.669016\n",
      "Epoch 284 : Train loss -> 0.596825 Test loss -> 0.607847 Train acc -> 0.680014 Test acc -> 0.668740\n",
      "Epoch 285 : Train loss -> 0.596754 Test loss -> 0.607813 Train acc -> 0.679955 Test acc -> 0.668740\n",
      "Epoch 286 : Train loss -> 0.596683 Test loss -> 0.607762 Train acc -> 0.679777 Test acc -> 0.668740\n",
      "Epoch 287 : Train loss -> 0.596613 Test loss -> 0.607713 Train acc -> 0.680014 Test acc -> 0.668878\n",
      "Epoch 288 : Train loss -> 0.596542 Test loss -> 0.607652 Train acc -> 0.680133 Test acc -> 0.669016\n",
      "Epoch 289 : Train loss -> 0.596472 Test loss -> 0.607627 Train acc -> 0.680073 Test acc -> 0.668878\n",
      "Epoch 290 : Train loss -> 0.596403 Test loss -> 0.607601 Train acc -> 0.679955 Test acc -> 0.668878\n",
      "Epoch 291 : Train loss -> 0.596334 Test loss -> 0.607559 Train acc -> 0.679837 Test acc -> 0.669016\n",
      "Epoch 292 : Train loss -> 0.596266 Test loss -> 0.607516 Train acc -> 0.679837 Test acc -> 0.669154\n",
      "Epoch 293 : Train loss -> 0.596198 Test loss -> 0.607463 Train acc -> 0.679837 Test acc -> 0.669154\n",
      "Epoch 294 : Train loss -> 0.596131 Test loss -> 0.607442 Train acc -> 0.680014 Test acc -> 0.668878\n",
      "Epoch 295 : Train loss -> 0.596063 Test loss -> 0.607383 Train acc -> 0.679837 Test acc -> 0.668878\n",
      "Epoch 296 : Train loss -> 0.595996 Test loss -> 0.607345 Train acc -> 0.680014 Test acc -> 0.668601\n",
      "Epoch 297 : Train loss -> 0.595929 Test loss -> 0.607309 Train acc -> 0.680192 Test acc -> 0.668878\n",
      "Epoch 298 : Train loss -> 0.595864 Test loss -> 0.607276 Train acc -> 0.680429 Test acc -> 0.668878\n",
      "Epoch 299 : Train loss -> 0.595796 Test loss -> 0.607225 Train acc -> 0.680607 Test acc -> 0.668740\n",
      "Epoch 300 : Train loss -> 0.595731 Test loss -> 0.607194 Train acc -> 0.680547 Test acc -> 0.668463\n",
      "Epoch 301 : Train loss -> 0.595666 Test loss -> 0.607148 Train acc -> 0.680547 Test acc -> 0.668325\n",
      "Epoch 302 : Train loss -> 0.595601 Test loss -> 0.607110 Train acc -> 0.680488 Test acc -> 0.668325\n",
      "Epoch 303 : Train loss -> 0.595537 Test loss -> 0.607053 Train acc -> 0.680429 Test acc -> 0.668187\n",
      "Epoch 304 : Train loss -> 0.595475 Test loss -> 0.607030 Train acc -> 0.680784 Test acc -> 0.668187\n",
      "Epoch 305 : Train loss -> 0.595411 Test loss -> 0.606992 Train acc -> 0.680666 Test acc -> 0.668187\n",
      "Epoch 306 : Train loss -> 0.595349 Test loss -> 0.606949 Train acc -> 0.680666 Test acc -> 0.668187\n",
      "Epoch 307 : Train loss -> 0.595286 Test loss -> 0.606916 Train acc -> 0.680607 Test acc -> 0.668187\n",
      "Epoch 308 : Train loss -> 0.595225 Test loss -> 0.606898 Train acc -> 0.681080 Test acc -> 0.668325\n",
      "Epoch 309 : Train loss -> 0.595163 Test loss -> 0.606863 Train acc -> 0.681140 Test acc -> 0.668187\n",
      "Epoch 310 : Train loss -> 0.595102 Test loss -> 0.606810 Train acc -> 0.681021 Test acc -> 0.668463\n",
      "Epoch 311 : Train loss -> 0.595042 Test loss -> 0.606775 Train acc -> 0.681258 Test acc -> 0.668325\n",
      "Epoch 312 : Train loss -> 0.594980 Test loss -> 0.606747 Train acc -> 0.681140 Test acc -> 0.668463\n",
      "Epoch 313 : Train loss -> 0.594919 Test loss -> 0.606719 Train acc -> 0.681140 Test acc -> 0.668463\n",
      "Epoch 314 : Train loss -> 0.594859 Test loss -> 0.606674 Train acc -> 0.681258 Test acc -> 0.668601\n",
      "Epoch 315 : Train loss -> 0.594798 Test loss -> 0.606647 Train acc -> 0.681199 Test acc -> 0.668740\n",
      "Epoch 316 : Train loss -> 0.594740 Test loss -> 0.606600 Train acc -> 0.680962 Test acc -> 0.668878\n",
      "Epoch 317 : Train loss -> 0.594680 Test loss -> 0.606564 Train acc -> 0.681021 Test acc -> 0.668878\n",
      "Epoch 318 : Train loss -> 0.594621 Test loss -> 0.606535 Train acc -> 0.680903 Test acc -> 0.669016\n",
      "Epoch 319 : Train loss -> 0.594562 Test loss -> 0.606502 Train acc -> 0.680843 Test acc -> 0.669016\n",
      "Epoch 320 : Train loss -> 0.594503 Test loss -> 0.606465 Train acc -> 0.680784 Test acc -> 0.669016\n",
      "Epoch 321 : Train loss -> 0.594445 Test loss -> 0.606421 Train acc -> 0.680666 Test acc -> 0.668878\n",
      "Epoch 322 : Train loss -> 0.594388 Test loss -> 0.606425 Train acc -> 0.680903 Test acc -> 0.669569\n",
      "Epoch 323 : Train loss -> 0.594331 Test loss -> 0.606398 Train acc -> 0.680962 Test acc -> 0.669569\n",
      "Epoch 324 : Train loss -> 0.594273 Test loss -> 0.606342 Train acc -> 0.680962 Test acc -> 0.669569\n",
      "Epoch 325 : Train loss -> 0.594215 Test loss -> 0.606312 Train acc -> 0.681080 Test acc -> 0.669569\n",
      "Epoch 326 : Train loss -> 0.594158 Test loss -> 0.606275 Train acc -> 0.681140 Test acc -> 0.669569\n",
      "Epoch 327 : Train loss -> 0.594102 Test loss -> 0.606249 Train acc -> 0.681258 Test acc -> 0.669707\n",
      "Epoch 328 : Train loss -> 0.594046 Test loss -> 0.606233 Train acc -> 0.681199 Test acc -> 0.669707\n",
      "Epoch 329 : Train loss -> 0.593988 Test loss -> 0.606176 Train acc -> 0.681199 Test acc -> 0.669707\n",
      "Epoch 330 : Train loss -> 0.593935 Test loss -> 0.606132 Train acc -> 0.681258 Test acc -> 0.669983\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 331 : Train loss -> 0.593877 Test loss -> 0.606115 Train acc -> 0.681377 Test acc -> 0.670122\n",
      "Epoch 332 : Train loss -> 0.593822 Test loss -> 0.606093 Train acc -> 0.681436 Test acc -> 0.670260\n",
      "Epoch 333 : Train loss -> 0.593766 Test loss -> 0.606056 Train acc -> 0.681436 Test acc -> 0.670122\n",
      "Epoch 334 : Train loss -> 0.593711 Test loss -> 0.606043 Train acc -> 0.681377 Test acc -> 0.670122\n",
      "Epoch 335 : Train loss -> 0.593658 Test loss -> 0.606003 Train acc -> 0.681436 Test acc -> 0.670122\n",
      "Epoch 336 : Train loss -> 0.593605 Test loss -> 0.605974 Train acc -> 0.681554 Test acc -> 0.670122\n",
      "Epoch 337 : Train loss -> 0.593550 Test loss -> 0.605956 Train acc -> 0.681495 Test acc -> 0.670122\n",
      "Epoch 338 : Train loss -> 0.593496 Test loss -> 0.605917 Train acc -> 0.681495 Test acc -> 0.670260\n",
      "Epoch 339 : Train loss -> 0.593442 Test loss -> 0.605884 Train acc -> 0.681791 Test acc -> 0.670398\n",
      "Epoch 340 : Train loss -> 0.593388 Test loss -> 0.605854 Train acc -> 0.681791 Test acc -> 0.670398\n",
      "Epoch 341 : Train loss -> 0.593335 Test loss -> 0.605826 Train acc -> 0.681850 Test acc -> 0.670398\n",
      "Epoch 342 : Train loss -> 0.593282 Test loss -> 0.605812 Train acc -> 0.681850 Test acc -> 0.670536\n",
      "Epoch 343 : Train loss -> 0.593228 Test loss -> 0.605781 Train acc -> 0.681732 Test acc -> 0.670813\n",
      "Epoch 344 : Train loss -> 0.593178 Test loss -> 0.605758 Train acc -> 0.681673 Test acc -> 0.670951\n",
      "Epoch 345 : Train loss -> 0.593126 Test loss -> 0.605717 Train acc -> 0.681910 Test acc -> 0.670951\n",
      "Epoch 346 : Train loss -> 0.593073 Test loss -> 0.605682 Train acc -> 0.681850 Test acc -> 0.670674\n",
      "Epoch 347 : Train loss -> 0.593023 Test loss -> 0.605653 Train acc -> 0.681910 Test acc -> 0.670951\n",
      "Epoch 348 : Train loss -> 0.592971 Test loss -> 0.605614 Train acc -> 0.681732 Test acc -> 0.670674\n",
      "Epoch 349 : Train loss -> 0.592922 Test loss -> 0.605621 Train acc -> 0.681850 Test acc -> 0.670951\n",
      "Epoch 350 : Train loss -> 0.592870 Test loss -> 0.605593 Train acc -> 0.682028 Test acc -> 0.670813\n",
      "Epoch 351 : Train loss -> 0.592820 Test loss -> 0.605553 Train acc -> 0.681969 Test acc -> 0.670674\n",
      "Epoch 352 : Train loss -> 0.592769 Test loss -> 0.605541 Train acc -> 0.682502 Test acc -> 0.670951\n",
      "Epoch 353 : Train loss -> 0.592719 Test loss -> 0.605494 Train acc -> 0.682384 Test acc -> 0.670674\n",
      "Epoch 354 : Train loss -> 0.592671 Test loss -> 0.605464 Train acc -> 0.682502 Test acc -> 0.670536\n",
      "Epoch 355 : Train loss -> 0.592621 Test loss -> 0.605455 Train acc -> 0.682798 Test acc -> 0.671227\n",
      "Epoch 356 : Train loss -> 0.592571 Test loss -> 0.605416 Train acc -> 0.682739 Test acc -> 0.671504\n",
      "Epoch 357 : Train loss -> 0.592522 Test loss -> 0.605399 Train acc -> 0.682739 Test acc -> 0.671365\n",
      "Epoch 358 : Train loss -> 0.592473 Test loss -> 0.605366 Train acc -> 0.682739 Test acc -> 0.671504\n",
      "Epoch 359 : Train loss -> 0.592424 Test loss -> 0.605342 Train acc -> 0.682739 Test acc -> 0.671227\n",
      "Epoch 360 : Train loss -> 0.592377 Test loss -> 0.605305 Train acc -> 0.682857 Test acc -> 0.671089\n",
      "Epoch 361 : Train loss -> 0.592327 Test loss -> 0.605295 Train acc -> 0.682917 Test acc -> 0.671089\n",
      "Epoch 362 : Train loss -> 0.592280 Test loss -> 0.605267 Train acc -> 0.682976 Test acc -> 0.671227\n",
      "Epoch 363 : Train loss -> 0.592233 Test loss -> 0.605253 Train acc -> 0.683154 Test acc -> 0.671089\n",
      "Epoch 364 : Train loss -> 0.592183 Test loss -> 0.605221 Train acc -> 0.683094 Test acc -> 0.671365\n",
      "Epoch 365 : Train loss -> 0.592136 Test loss -> 0.605202 Train acc -> 0.683391 Test acc -> 0.671227\n",
      "Epoch 366 : Train loss -> 0.592088 Test loss -> 0.605179 Train acc -> 0.683272 Test acc -> 0.671780\n",
      "Epoch 367 : Train loss -> 0.592041 Test loss -> 0.605138 Train acc -> 0.683509 Test acc -> 0.671227\n",
      "Epoch 368 : Train loss -> 0.591993 Test loss -> 0.605091 Train acc -> 0.683450 Test acc -> 0.671365\n",
      "Epoch 369 : Train loss -> 0.591946 Test loss -> 0.605072 Train acc -> 0.683568 Test acc -> 0.671227\n",
      "Epoch 370 : Train loss -> 0.591898 Test loss -> 0.605051 Train acc -> 0.683450 Test acc -> 0.670951\n",
      "Epoch 371 : Train loss -> 0.591850 Test loss -> 0.605020 Train acc -> 0.683509 Test acc -> 0.671227\n",
      "Epoch 372 : Train loss -> 0.591803 Test loss -> 0.605031 Train acc -> 0.683628 Test acc -> 0.671365\n",
      "Epoch 373 : Train loss -> 0.591756 Test loss -> 0.604999 Train acc -> 0.683568 Test acc -> 0.671365\n",
      "Epoch 374 : Train loss -> 0.591710 Test loss -> 0.604968 Train acc -> 0.683450 Test acc -> 0.671089\n",
      "Epoch 375 : Train loss -> 0.591663 Test loss -> 0.604951 Train acc -> 0.683687 Test acc -> 0.671365\n",
      "Epoch 376 : Train loss -> 0.591616 Test loss -> 0.604918 Train acc -> 0.683628 Test acc -> 0.670813\n",
      "Epoch 377 : Train loss -> 0.591571 Test loss -> 0.604893 Train acc -> 0.683687 Test acc -> 0.670951\n",
      "Epoch 378 : Train loss -> 0.591524 Test loss -> 0.604865 Train acc -> 0.683805 Test acc -> 0.670813\n",
      "Epoch 379 : Train loss -> 0.591478 Test loss -> 0.604843 Train acc -> 0.683864 Test acc -> 0.671227\n",
      "Epoch 380 : Train loss -> 0.591431 Test loss -> 0.604814 Train acc -> 0.683864 Test acc -> 0.671089\n",
      "Epoch 381 : Train loss -> 0.591384 Test loss -> 0.604784 Train acc -> 0.683687 Test acc -> 0.671227\n",
      "Epoch 382 : Train loss -> 0.591338 Test loss -> 0.604781 Train acc -> 0.683687 Test acc -> 0.671089\n",
      "Epoch 383 : Train loss -> 0.591293 Test loss -> 0.604745 Train acc -> 0.683864 Test acc -> 0.671365\n",
      "Epoch 384 : Train loss -> 0.591246 Test loss -> 0.604713 Train acc -> 0.683746 Test acc -> 0.671227\n",
      "Epoch 385 : Train loss -> 0.591200 Test loss -> 0.604689 Train acc -> 0.683450 Test acc -> 0.671089\n",
      "Epoch 386 : Train loss -> 0.591155 Test loss -> 0.604674 Train acc -> 0.683450 Test acc -> 0.671089\n",
      "Epoch 387 : Train loss -> 0.591109 Test loss -> 0.604649 Train acc -> 0.683450 Test acc -> 0.670951\n",
      "Epoch 388 : Train loss -> 0.591064 Test loss -> 0.604632 Train acc -> 0.683509 Test acc -> 0.671227\n",
      "Epoch 389 : Train loss -> 0.591020 Test loss -> 0.604616 Train acc -> 0.683568 Test acc -> 0.670951\n",
      "Epoch 390 : Train loss -> 0.590976 Test loss -> 0.604583 Train acc -> 0.683568 Test acc -> 0.671089\n",
      "Epoch 391 : Train loss -> 0.590932 Test loss -> 0.604557 Train acc -> 0.683687 Test acc -> 0.671089\n",
      "Epoch 392 : Train loss -> 0.590889 Test loss -> 0.604536 Train acc -> 0.683628 Test acc -> 0.671365\n",
      "Epoch 393 : Train loss -> 0.590845 Test loss -> 0.604513 Train acc -> 0.683687 Test acc -> 0.671227\n",
      "Epoch 394 : Train loss -> 0.590802 Test loss -> 0.604487 Train acc -> 0.683805 Test acc -> 0.671227\n",
      "Epoch 395 : Train loss -> 0.590758 Test loss -> 0.604462 Train acc -> 0.683864 Test acc -> 0.671227\n",
      "Epoch 396 : Train loss -> 0.590715 Test loss -> 0.604438 Train acc -> 0.683805 Test acc -> 0.671365\n",
      "Epoch 397 : Train loss -> 0.590672 Test loss -> 0.604433 Train acc -> 0.683924 Test acc -> 0.671365\n",
      "Epoch 398 : Train loss -> 0.590629 Test loss -> 0.604405 Train acc -> 0.684042 Test acc -> 0.671227\n",
      "Epoch 399 : Train loss -> 0.590587 Test loss -> 0.604377 Train acc -> 0.684338 Test acc -> 0.671504\n",
      "Epoch 400 : Train loss -> 0.590545 Test loss -> 0.604365 Train acc -> 0.684161 Test acc -> 0.671504\n",
      "Epoch 401 : Train loss -> 0.590502 Test loss -> 0.604342 Train acc -> 0.683983 Test acc -> 0.671504\n",
      "Epoch 402 : Train loss -> 0.590460 Test loss -> 0.604317 Train acc -> 0.684457 Test acc -> 0.671365\n",
      "Epoch 403 : Train loss -> 0.590419 Test loss -> 0.604296 Train acc -> 0.684457 Test acc -> 0.671642\n",
      "Epoch 404 : Train loss -> 0.590377 Test loss -> 0.604269 Train acc -> 0.684279 Test acc -> 0.671504\n",
      "Epoch 405 : Train loss -> 0.590336 Test loss -> 0.604248 Train acc -> 0.684161 Test acc -> 0.671504\n",
      "Epoch 406 : Train loss -> 0.590295 Test loss -> 0.604222 Train acc -> 0.684457 Test acc -> 0.671780\n",
      "Epoch 407 : Train loss -> 0.590255 Test loss -> 0.604227 Train acc -> 0.683983 Test acc -> 0.671504\n",
      "Epoch 408 : Train loss -> 0.590214 Test loss -> 0.604179 Train acc -> 0.684457 Test acc -> 0.671642\n",
      "Epoch 409 : Train loss -> 0.590174 Test loss -> 0.604167 Train acc -> 0.684457 Test acc -> 0.671642\n",
      "Epoch 410 : Train loss -> 0.590133 Test loss -> 0.604149 Train acc -> 0.684457 Test acc -> 0.671642\n",
      "Epoch 411 : Train loss -> 0.590093 Test loss -> 0.604129 Train acc -> 0.684338 Test acc -> 0.671642\n",
      "Epoch 412 : Train loss -> 0.590053 Test loss -> 0.604115 Train acc -> 0.684457 Test acc -> 0.671504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 413 : Train loss -> 0.590013 Test loss -> 0.604086 Train acc -> 0.684338 Test acc -> 0.671504\n",
      "Epoch 414 : Train loss -> 0.589974 Test loss -> 0.604075 Train acc -> 0.684457 Test acc -> 0.671504\n",
      "Epoch 415 : Train loss -> 0.589934 Test loss -> 0.604046 Train acc -> 0.684575 Test acc -> 0.671504\n",
      "Epoch 416 : Train loss -> 0.589894 Test loss -> 0.604038 Train acc -> 0.684516 Test acc -> 0.671780\n",
      "Epoch 417 : Train loss -> 0.589856 Test loss -> 0.604000 Train acc -> 0.684457 Test acc -> 0.671918\n",
      "Epoch 418 : Train loss -> 0.589819 Test loss -> 0.603990 Train acc -> 0.684457 Test acc -> 0.671918\n",
      "Epoch 419 : Train loss -> 0.589779 Test loss -> 0.603961 Train acc -> 0.684398 Test acc -> 0.672056\n",
      "Epoch 420 : Train loss -> 0.589741 Test loss -> 0.603957 Train acc -> 0.684635 Test acc -> 0.671918\n",
      "Epoch 421 : Train loss -> 0.589703 Test loss -> 0.603919 Train acc -> 0.684338 Test acc -> 0.672195\n",
      "Epoch 422 : Train loss -> 0.589665 Test loss -> 0.603920 Train acc -> 0.684753 Test acc -> 0.672195\n",
      "Epoch 423 : Train loss -> 0.589627 Test loss -> 0.603885 Train acc -> 0.684457 Test acc -> 0.672471\n",
      "Epoch 424 : Train loss -> 0.589589 Test loss -> 0.603877 Train acc -> 0.684457 Test acc -> 0.672333\n",
      "Epoch 425 : Train loss -> 0.589552 Test loss -> 0.603848 Train acc -> 0.684457 Test acc -> 0.672609\n",
      "Epoch 426 : Train loss -> 0.589516 Test loss -> 0.603839 Train acc -> 0.684575 Test acc -> 0.672609\n",
      "Epoch 427 : Train loss -> 0.589479 Test loss -> 0.603821 Train acc -> 0.684635 Test acc -> 0.672609\n",
      "Epoch 428 : Train loss -> 0.589441 Test loss -> 0.603806 Train acc -> 0.684516 Test acc -> 0.672609\n",
      "Epoch 429 : Train loss -> 0.589405 Test loss -> 0.603781 Train acc -> 0.684575 Test acc -> 0.672609\n",
      "Epoch 430 : Train loss -> 0.589368 Test loss -> 0.603784 Train acc -> 0.684871 Test acc -> 0.672747\n",
      "Epoch 431 : Train loss -> 0.589332 Test loss -> 0.603758 Train acc -> 0.684871 Test acc -> 0.672747\n",
      "Epoch 432 : Train loss -> 0.589296 Test loss -> 0.603721 Train acc -> 0.684753 Test acc -> 0.672747\n",
      "Epoch 433 : Train loss -> 0.589258 Test loss -> 0.603718 Train acc -> 0.684931 Test acc -> 0.672747\n",
      "Epoch 434 : Train loss -> 0.589222 Test loss -> 0.603700 Train acc -> 0.684931 Test acc -> 0.672886\n",
      "Epoch 435 : Train loss -> 0.589186 Test loss -> 0.603682 Train acc -> 0.685049 Test acc -> 0.672886\n",
      "Epoch 436 : Train loss -> 0.589149 Test loss -> 0.603677 Train acc -> 0.685049 Test acc -> 0.672747\n",
      "Epoch 437 : Train loss -> 0.589113 Test loss -> 0.603639 Train acc -> 0.684931 Test acc -> 0.672886\n",
      "Epoch 438 : Train loss -> 0.589077 Test loss -> 0.603636 Train acc -> 0.685227 Test acc -> 0.672886\n",
      "Epoch 439 : Train loss -> 0.589041 Test loss -> 0.603608 Train acc -> 0.685168 Test acc -> 0.672747\n",
      "Epoch 440 : Train loss -> 0.589006 Test loss -> 0.603582 Train acc -> 0.685227 Test acc -> 0.672747\n",
      "Epoch 441 : Train loss -> 0.588970 Test loss -> 0.603576 Train acc -> 0.685405 Test acc -> 0.672747\n",
      "Epoch 442 : Train loss -> 0.588934 Test loss -> 0.603565 Train acc -> 0.685405 Test acc -> 0.672609\n",
      "Epoch 443 : Train loss -> 0.588899 Test loss -> 0.603540 Train acc -> 0.685464 Test acc -> 0.672609\n",
      "Epoch 444 : Train loss -> 0.588863 Test loss -> 0.603523 Train acc -> 0.685464 Test acc -> 0.672471\n",
      "Epoch 445 : Train loss -> 0.588828 Test loss -> 0.603517 Train acc -> 0.685464 Test acc -> 0.672609\n",
      "Epoch 446 : Train loss -> 0.588794 Test loss -> 0.603495 Train acc -> 0.685464 Test acc -> 0.672747\n",
      "Epoch 447 : Train loss -> 0.588760 Test loss -> 0.603472 Train acc -> 0.685405 Test acc -> 0.672747\n",
      "Epoch 448 : Train loss -> 0.588724 Test loss -> 0.603456 Train acc -> 0.685405 Test acc -> 0.672886\n",
      "Epoch 449 : Train loss -> 0.588689 Test loss -> 0.603443 Train acc -> 0.685701 Test acc -> 0.673024\n",
      "Epoch 450 : Train loss -> 0.588655 Test loss -> 0.603446 Train acc -> 0.685878 Test acc -> 0.673300\n",
      "Epoch 451 : Train loss -> 0.588622 Test loss -> 0.603412 Train acc -> 0.685878 Test acc -> 0.673162\n",
      "Epoch 452 : Train loss -> 0.588588 Test loss -> 0.603401 Train acc -> 0.685938 Test acc -> 0.673024\n",
      "Epoch 453 : Train loss -> 0.588553 Test loss -> 0.603387 Train acc -> 0.685997 Test acc -> 0.673162\n",
      "Epoch 454 : Train loss -> 0.588519 Test loss -> 0.603376 Train acc -> 0.686234 Test acc -> 0.673300\n",
      "Epoch 455 : Train loss -> 0.588486 Test loss -> 0.603356 Train acc -> 0.686234 Test acc -> 0.673300\n",
      "Epoch 456 : Train loss -> 0.588453 Test loss -> 0.603363 Train acc -> 0.686412 Test acc -> 0.673300\n",
      "Epoch 457 : Train loss -> 0.588419 Test loss -> 0.603337 Train acc -> 0.686412 Test acc -> 0.673438\n",
      "Epoch 458 : Train loss -> 0.588386 Test loss -> 0.603307 Train acc -> 0.686352 Test acc -> 0.673300\n",
      "Epoch 459 : Train loss -> 0.588352 Test loss -> 0.603309 Train acc -> 0.686708 Test acc -> 0.673300\n",
      "Epoch 460 : Train loss -> 0.588319 Test loss -> 0.603290 Train acc -> 0.686648 Test acc -> 0.673438\n",
      "Epoch 461 : Train loss -> 0.588286 Test loss -> 0.603255 Train acc -> 0.686530 Test acc -> 0.673577\n",
      "Epoch 462 : Train loss -> 0.588253 Test loss -> 0.603243 Train acc -> 0.686589 Test acc -> 0.673577\n",
      "Epoch 463 : Train loss -> 0.588221 Test loss -> 0.603233 Train acc -> 0.686708 Test acc -> 0.673438\n",
      "Epoch 464 : Train loss -> 0.588188 Test loss -> 0.603212 Train acc -> 0.686826 Test acc -> 0.673577\n",
      "Epoch 465 : Train loss -> 0.588155 Test loss -> 0.603196 Train acc -> 0.686885 Test acc -> 0.673715\n",
      "Epoch 466 : Train loss -> 0.588123 Test loss -> 0.603186 Train acc -> 0.686767 Test acc -> 0.673438\n",
      "Epoch 467 : Train loss -> 0.588091 Test loss -> 0.603167 Train acc -> 0.686945 Test acc -> 0.673438\n",
      "Epoch 468 : Train loss -> 0.588058 Test loss -> 0.603145 Train acc -> 0.686826 Test acc -> 0.673438\n",
      "Epoch 469 : Train loss -> 0.588027 Test loss -> 0.603132 Train acc -> 0.686826 Test acc -> 0.673438\n",
      "Epoch 470 : Train loss -> 0.587995 Test loss -> 0.603124 Train acc -> 0.686767 Test acc -> 0.673300\n",
      "Epoch 471 : Train loss -> 0.587962 Test loss -> 0.603113 Train acc -> 0.686708 Test acc -> 0.673438\n",
      "Epoch 472 : Train loss -> 0.587929 Test loss -> 0.603088 Train acc -> 0.686530 Test acc -> 0.673577\n",
      "Epoch 473 : Train loss -> 0.587897 Test loss -> 0.603079 Train acc -> 0.686648 Test acc -> 0.673577\n",
      "Epoch 474 : Train loss -> 0.587866 Test loss -> 0.603063 Train acc -> 0.686826 Test acc -> 0.673715\n",
      "Epoch 475 : Train loss -> 0.587834 Test loss -> 0.603053 Train acc -> 0.686826 Test acc -> 0.673715\n",
      "Epoch 476 : Train loss -> 0.587802 Test loss -> 0.603035 Train acc -> 0.686708 Test acc -> 0.673853\n",
      "Epoch 477 : Train loss -> 0.587771 Test loss -> 0.603014 Train acc -> 0.686708 Test acc -> 0.673438\n",
      "Epoch 478 : Train loss -> 0.587739 Test loss -> 0.603000 Train acc -> 0.686708 Test acc -> 0.673300\n",
      "Epoch 479 : Train loss -> 0.587707 Test loss -> 0.602987 Train acc -> 0.686589 Test acc -> 0.673577\n",
      "Epoch 480 : Train loss -> 0.587676 Test loss -> 0.602961 Train acc -> 0.686826 Test acc -> 0.673853\n",
      "Epoch 481 : Train loss -> 0.587643 Test loss -> 0.602961 Train acc -> 0.686885 Test acc -> 0.673853\n",
      "Epoch 482 : Train loss -> 0.587613 Test loss -> 0.602936 Train acc -> 0.686885 Test acc -> 0.673853\n",
      "Epoch 483 : Train loss -> 0.587582 Test loss -> 0.602934 Train acc -> 0.687063 Test acc -> 0.673577\n",
      "Epoch 484 : Train loss -> 0.587551 Test loss -> 0.602923 Train acc -> 0.687122 Test acc -> 0.673577\n",
      "Epoch 485 : Train loss -> 0.587520 Test loss -> 0.602910 Train acc -> 0.687004 Test acc -> 0.673715\n",
      "Epoch 486 : Train loss -> 0.587489 Test loss -> 0.602905 Train acc -> 0.687241 Test acc -> 0.673853\n",
      "Epoch 487 : Train loss -> 0.587458 Test loss -> 0.602877 Train acc -> 0.687359 Test acc -> 0.673577\n",
      "Epoch 488 : Train loss -> 0.587427 Test loss -> 0.602874 Train acc -> 0.687300 Test acc -> 0.673577\n",
      "Epoch 489 : Train loss -> 0.587397 Test loss -> 0.602857 Train acc -> 0.687182 Test acc -> 0.673715\n",
      "Epoch 490 : Train loss -> 0.587367 Test loss -> 0.602846 Train acc -> 0.687182 Test acc -> 0.673715\n",
      "Epoch 491 : Train loss -> 0.587337 Test loss -> 0.602827 Train acc -> 0.687122 Test acc -> 0.673577\n",
      "Epoch 492 : Train loss -> 0.587307 Test loss -> 0.602806 Train acc -> 0.687122 Test acc -> 0.673853\n",
      "Epoch 493 : Train loss -> 0.587277 Test loss -> 0.602801 Train acc -> 0.687122 Test acc -> 0.673853\n",
      "Epoch 494 : Train loss -> 0.587247 Test loss -> 0.602792 Train acc -> 0.687063 Test acc -> 0.673577\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 495 : Train loss -> 0.587218 Test loss -> 0.602764 Train acc -> 0.686945 Test acc -> 0.673853\n",
      "Epoch 496 : Train loss -> 0.587188 Test loss -> 0.602758 Train acc -> 0.687122 Test acc -> 0.673577\n",
      "Epoch 497 : Train loss -> 0.587159 Test loss -> 0.602744 Train acc -> 0.687182 Test acc -> 0.673853\n",
      "Epoch 498 : Train loss -> 0.587130 Test loss -> 0.602741 Train acc -> 0.687182 Test acc -> 0.673715\n",
      "Epoch 499 : Train loss -> 0.587101 Test loss -> 0.602736 Train acc -> 0.687122 Test acc -> 0.673991\n",
      "Epoch 500 : Train loss -> 0.587071 Test loss -> 0.602710 Train acc -> 0.687122 Test acc -> 0.673991\n",
      "Epoch 501 : Train loss -> 0.587042 Test loss -> 0.602697 Train acc -> 0.687063 Test acc -> 0.674129\n",
      "Epoch 502 : Train loss -> 0.587014 Test loss -> 0.602683 Train acc -> 0.687004 Test acc -> 0.673991\n",
      "Epoch 503 : Train loss -> 0.586985 Test loss -> 0.602679 Train acc -> 0.687122 Test acc -> 0.673991\n",
      "Epoch 504 : Train loss -> 0.586956 Test loss -> 0.602662 Train acc -> 0.687004 Test acc -> 0.673991\n",
      "Epoch 505 : Train loss -> 0.586928 Test loss -> 0.602657 Train acc -> 0.687004 Test acc -> 0.673991\n",
      "Epoch 506 : Train loss -> 0.586900 Test loss -> 0.602632 Train acc -> 0.686945 Test acc -> 0.673715\n",
      "Epoch 507 : Train loss -> 0.586872 Test loss -> 0.602615 Train acc -> 0.686945 Test acc -> 0.673715\n",
      "Epoch 508 : Train loss -> 0.586845 Test loss -> 0.602613 Train acc -> 0.686885 Test acc -> 0.674268\n",
      "Epoch 509 : Train loss -> 0.586816 Test loss -> 0.602605 Train acc -> 0.686885 Test acc -> 0.674268\n",
      "Epoch 510 : Train loss -> 0.586789 Test loss -> 0.602594 Train acc -> 0.687004 Test acc -> 0.674129\n",
      "Epoch 511 : Train loss -> 0.586761 Test loss -> 0.602584 Train acc -> 0.686826 Test acc -> 0.673853\n",
      "Epoch 512 : Train loss -> 0.586734 Test loss -> 0.602570 Train acc -> 0.686945 Test acc -> 0.673853\n",
      "Epoch 513 : Train loss -> 0.586706 Test loss -> 0.602555 Train acc -> 0.686826 Test acc -> 0.673715\n",
      "Epoch 514 : Train loss -> 0.586678 Test loss -> 0.602547 Train acc -> 0.686945 Test acc -> 0.673853\n",
      "Epoch 515 : Train loss -> 0.586650 Test loss -> 0.602535 Train acc -> 0.686945 Test acc -> 0.673577\n",
      "Epoch 516 : Train loss -> 0.586623 Test loss -> 0.602524 Train acc -> 0.686885 Test acc -> 0.673715\n",
      "Epoch 517 : Train loss -> 0.586596 Test loss -> 0.602505 Train acc -> 0.686945 Test acc -> 0.673715\n",
      "Epoch 518 : Train loss -> 0.586569 Test loss -> 0.602500 Train acc -> 0.687004 Test acc -> 0.673853\n",
      "Epoch 519 : Train loss -> 0.586541 Test loss -> 0.602484 Train acc -> 0.687063 Test acc -> 0.673715\n",
      "Epoch 520 : Train loss -> 0.586515 Test loss -> 0.602470 Train acc -> 0.687182 Test acc -> 0.673715\n",
      "Epoch 521 : Train loss -> 0.586488 Test loss -> 0.602450 Train acc -> 0.687241 Test acc -> 0.673715\n",
      "Epoch 522 : Train loss -> 0.586461 Test loss -> 0.602441 Train acc -> 0.687122 Test acc -> 0.673853\n",
      "Epoch 523 : Train loss -> 0.586434 Test loss -> 0.602435 Train acc -> 0.687182 Test acc -> 0.673991\n",
      "Epoch 524 : Train loss -> 0.586408 Test loss -> 0.602427 Train acc -> 0.687241 Test acc -> 0.673991\n",
      "Epoch 525 : Train loss -> 0.586381 Test loss -> 0.602408 Train acc -> 0.687300 Test acc -> 0.673853\n",
      "Epoch 526 : Train loss -> 0.586355 Test loss -> 0.602394 Train acc -> 0.687241 Test acc -> 0.673853\n",
      "Epoch 527 : Train loss -> 0.586329 Test loss -> 0.602382 Train acc -> 0.687300 Test acc -> 0.673715\n",
      "Epoch 528 : Train loss -> 0.586302 Test loss -> 0.602386 Train acc -> 0.687419 Test acc -> 0.673991\n",
      "Epoch 529 : Train loss -> 0.586275 Test loss -> 0.602383 Train acc -> 0.687596 Test acc -> 0.673853\n",
      "Epoch 530 : Train loss -> 0.586250 Test loss -> 0.602379 Train acc -> 0.687715 Test acc -> 0.673715\n",
      "Epoch 531 : Train loss -> 0.586222 Test loss -> 0.602359 Train acc -> 0.687715 Test acc -> 0.673715\n",
      "Epoch 532 : Train loss -> 0.586197 Test loss -> 0.602349 Train acc -> 0.687596 Test acc -> 0.673715\n",
      "Epoch 533 : Train loss -> 0.586171 Test loss -> 0.602328 Train acc -> 0.687656 Test acc -> 0.673715\n",
      "Epoch 534 : Train loss -> 0.586145 Test loss -> 0.602329 Train acc -> 0.687715 Test acc -> 0.673715\n",
      "Epoch 535 : Train loss -> 0.586119 Test loss -> 0.602313 Train acc -> 0.687774 Test acc -> 0.673991\n",
      "Epoch 536 : Train loss -> 0.586093 Test loss -> 0.602287 Train acc -> 0.687774 Test acc -> 0.674129\n",
      "Epoch 537 : Train loss -> 0.586067 Test loss -> 0.602273 Train acc -> 0.687774 Test acc -> 0.673991\n",
      "Epoch 538 : Train loss -> 0.586040 Test loss -> 0.602265 Train acc -> 0.687833 Test acc -> 0.674129\n",
      "Epoch 539 : Train loss -> 0.586015 Test loss -> 0.602255 Train acc -> 0.687952 Test acc -> 0.674129\n",
      "Epoch 540 : Train loss -> 0.585990 Test loss -> 0.602252 Train acc -> 0.688070 Test acc -> 0.674406\n",
      "Epoch 541 : Train loss -> 0.585965 Test loss -> 0.602219 Train acc -> 0.687952 Test acc -> 0.674129\n",
      "Epoch 542 : Train loss -> 0.585940 Test loss -> 0.602202 Train acc -> 0.688070 Test acc -> 0.674129\n",
      "Epoch 543 : Train loss -> 0.585914 Test loss -> 0.602218 Train acc -> 0.688070 Test acc -> 0.674129\n",
      "Epoch 544 : Train loss -> 0.585889 Test loss -> 0.602195 Train acc -> 0.688248 Test acc -> 0.674268\n",
      "Epoch 545 : Train loss -> 0.585863 Test loss -> 0.602191 Train acc -> 0.688366 Test acc -> 0.674268\n",
      "Epoch 546 : Train loss -> 0.585839 Test loss -> 0.602190 Train acc -> 0.688366 Test acc -> 0.674544\n",
      "Epoch 547 : Train loss -> 0.585814 Test loss -> 0.602170 Train acc -> 0.688307 Test acc -> 0.674544\n",
      "Epoch 548 : Train loss -> 0.585789 Test loss -> 0.602174 Train acc -> 0.688426 Test acc -> 0.674129\n",
      "Epoch 549 : Train loss -> 0.585763 Test loss -> 0.602149 Train acc -> 0.688426 Test acc -> 0.674129\n",
      "Epoch 550 : Train loss -> 0.585739 Test loss -> 0.602125 Train acc -> 0.688485 Test acc -> 0.674268\n",
      "Epoch 551 : Train loss -> 0.585714 Test loss -> 0.602123 Train acc -> 0.688485 Test acc -> 0.674129\n",
      "Epoch 552 : Train loss -> 0.585689 Test loss -> 0.602116 Train acc -> 0.688485 Test acc -> 0.674129\n",
      "Epoch 553 : Train loss -> 0.585664 Test loss -> 0.602104 Train acc -> 0.688722 Test acc -> 0.674129\n",
      "Epoch 554 : Train loss -> 0.585639 Test loss -> 0.602086 Train acc -> 0.688544 Test acc -> 0.674406\n",
      "Epoch 555 : Train loss -> 0.585615 Test loss -> 0.602079 Train acc -> 0.688662 Test acc -> 0.674268\n",
      "Epoch 556 : Train loss -> 0.585591 Test loss -> 0.602068 Train acc -> 0.688603 Test acc -> 0.674268\n",
      "Epoch 557 : Train loss -> 0.585566 Test loss -> 0.602065 Train acc -> 0.688662 Test acc -> 0.674544\n",
      "Epoch 558 : Train loss -> 0.585541 Test loss -> 0.602049 Train acc -> 0.688544 Test acc -> 0.674544\n",
      "Epoch 559 : Train loss -> 0.585517 Test loss -> 0.602053 Train acc -> 0.688959 Test acc -> 0.674406\n",
      "Epoch 560 : Train loss -> 0.585492 Test loss -> 0.602027 Train acc -> 0.688662 Test acc -> 0.674544\n",
      "Epoch 561 : Train loss -> 0.585467 Test loss -> 0.602025 Train acc -> 0.688840 Test acc -> 0.674544\n",
      "Epoch 562 : Train loss -> 0.585443 Test loss -> 0.602021 Train acc -> 0.688840 Test acc -> 0.674406\n",
      "Epoch 563 : Train loss -> 0.585419 Test loss -> 0.602002 Train acc -> 0.688899 Test acc -> 0.674544\n",
      "Epoch 564 : Train loss -> 0.585394 Test loss -> 0.601989 Train acc -> 0.688899 Test acc -> 0.674544\n",
      "Epoch 565 : Train loss -> 0.585370 Test loss -> 0.601979 Train acc -> 0.688781 Test acc -> 0.674544\n",
      "Epoch 566 : Train loss -> 0.585346 Test loss -> 0.601971 Train acc -> 0.688899 Test acc -> 0.674544\n",
      "Epoch 567 : Train loss -> 0.585322 Test loss -> 0.601949 Train acc -> 0.688899 Test acc -> 0.674544\n",
      "Epoch 568 : Train loss -> 0.585297 Test loss -> 0.601947 Train acc -> 0.688840 Test acc -> 0.674406\n",
      "Epoch 569 : Train loss -> 0.585273 Test loss -> 0.601941 Train acc -> 0.688781 Test acc -> 0.674268\n",
      "Epoch 570 : Train loss -> 0.585249 Test loss -> 0.601938 Train acc -> 0.688781 Test acc -> 0.674268\n",
      "Epoch 571 : Train loss -> 0.585225 Test loss -> 0.601923 Train acc -> 0.688840 Test acc -> 0.674406\n",
      "Epoch 572 : Train loss -> 0.585201 Test loss -> 0.601913 Train acc -> 0.688840 Test acc -> 0.674406\n",
      "Epoch 573 : Train loss -> 0.585177 Test loss -> 0.601899 Train acc -> 0.688840 Test acc -> 0.674406\n",
      "Epoch 574 : Train loss -> 0.585154 Test loss -> 0.601898 Train acc -> 0.688840 Test acc -> 0.673991\n",
      "Epoch 575 : Train loss -> 0.585130 Test loss -> 0.601878 Train acc -> 0.688899 Test acc -> 0.674129\n",
      "Epoch 576 : Train loss -> 0.585107 Test loss -> 0.601882 Train acc -> 0.688899 Test acc -> 0.674129\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 577 : Train loss -> 0.585082 Test loss -> 0.601859 Train acc -> 0.688899 Test acc -> 0.674268\n",
      "Epoch 578 : Train loss -> 0.585060 Test loss -> 0.601868 Train acc -> 0.688899 Test acc -> 0.674268\n",
      "Epoch 579 : Train loss -> 0.585036 Test loss -> 0.601857 Train acc -> 0.689136 Test acc -> 0.674129\n",
      "Epoch 580 : Train loss -> 0.585012 Test loss -> 0.601856 Train acc -> 0.688959 Test acc -> 0.673991\n",
      "Epoch 581 : Train loss -> 0.584989 Test loss -> 0.601825 Train acc -> 0.689255 Test acc -> 0.674129\n",
      "Epoch 582 : Train loss -> 0.584966 Test loss -> 0.601830 Train acc -> 0.689077 Test acc -> 0.674129\n",
      "Epoch 583 : Train loss -> 0.584943 Test loss -> 0.601817 Train acc -> 0.689018 Test acc -> 0.674268\n",
      "Epoch 584 : Train loss -> 0.584920 Test loss -> 0.601805 Train acc -> 0.689018 Test acc -> 0.674406\n",
      "Epoch 585 : Train loss -> 0.584897 Test loss -> 0.601802 Train acc -> 0.689255 Test acc -> 0.674544\n",
      "Epoch 586 : Train loss -> 0.584874 Test loss -> 0.601783 Train acc -> 0.689196 Test acc -> 0.674406\n",
      "Epoch 587 : Train loss -> 0.584852 Test loss -> 0.601780 Train acc -> 0.689255 Test acc -> 0.674268\n",
      "Epoch 588 : Train loss -> 0.584830 Test loss -> 0.601772 Train acc -> 0.689373 Test acc -> 0.674268\n",
      "Epoch 589 : Train loss -> 0.584807 Test loss -> 0.601754 Train acc -> 0.689551 Test acc -> 0.674406\n",
      "Epoch 590 : Train loss -> 0.584784 Test loss -> 0.601753 Train acc -> 0.689610 Test acc -> 0.674406\n",
      "Epoch 591 : Train loss -> 0.584762 Test loss -> 0.601733 Train acc -> 0.689729 Test acc -> 0.674268\n",
      "Epoch 592 : Train loss -> 0.584740 Test loss -> 0.601732 Train acc -> 0.689610 Test acc -> 0.674406\n",
      "Epoch 593 : Train loss -> 0.584717 Test loss -> 0.601728 Train acc -> 0.689669 Test acc -> 0.674544\n",
      "Epoch 594 : Train loss -> 0.584694 Test loss -> 0.601723 Train acc -> 0.689729 Test acc -> 0.674268\n",
      "Epoch 595 : Train loss -> 0.584672 Test loss -> 0.601731 Train acc -> 0.689551 Test acc -> 0.674682\n",
      "Epoch 596 : Train loss -> 0.584650 Test loss -> 0.601711 Train acc -> 0.689669 Test acc -> 0.674682\n",
      "Epoch 597 : Train loss -> 0.584627 Test loss -> 0.601710 Train acc -> 0.689433 Test acc -> 0.674820\n",
      "Epoch 598 : Train loss -> 0.584604 Test loss -> 0.601695 Train acc -> 0.689669 Test acc -> 0.674682\n",
      "Epoch 599 : Train loss -> 0.584583 Test loss -> 0.601682 Train acc -> 0.689551 Test acc -> 0.674682\n",
      "Epoch 600 : Train loss -> 0.584560 Test loss -> 0.601688 Train acc -> 0.689492 Test acc -> 0.674959\n",
      "Epoch 601 : Train loss -> 0.584538 Test loss -> 0.601680 Train acc -> 0.689492 Test acc -> 0.674959\n",
      "Epoch 602 : Train loss -> 0.584515 Test loss -> 0.601665 Train acc -> 0.689610 Test acc -> 0.674820\n",
      "Epoch 603 : Train loss -> 0.584493 Test loss -> 0.601662 Train acc -> 0.689433 Test acc -> 0.674959\n",
      "Epoch 604 : Train loss -> 0.584471 Test loss -> 0.601643 Train acc -> 0.689669 Test acc -> 0.674268\n",
      "Epoch 605 : Train loss -> 0.584448 Test loss -> 0.601642 Train acc -> 0.689492 Test acc -> 0.674820\n",
      "Epoch 606 : Train loss -> 0.584426 Test loss -> 0.601644 Train acc -> 0.689610 Test acc -> 0.674544\n",
      "Epoch 607 : Train loss -> 0.584405 Test loss -> 0.601613 Train acc -> 0.689669 Test acc -> 0.674682\n",
      "Epoch 608 : Train loss -> 0.584383 Test loss -> 0.601610 Train acc -> 0.689669 Test acc -> 0.674406\n",
      "Epoch 609 : Train loss -> 0.584360 Test loss -> 0.601601 Train acc -> 0.689729 Test acc -> 0.674820\n",
      "Epoch 610 : Train loss -> 0.584338 Test loss -> 0.601599 Train acc -> 0.689729 Test acc -> 0.675097\n",
      "Epoch 611 : Train loss -> 0.584316 Test loss -> 0.601586 Train acc -> 0.689966 Test acc -> 0.675097\n",
      "Epoch 612 : Train loss -> 0.584294 Test loss -> 0.601573 Train acc -> 0.689788 Test acc -> 0.674959\n",
      "Epoch 613 : Train loss -> 0.584272 Test loss -> 0.601571 Train acc -> 0.689729 Test acc -> 0.675097\n",
      "Epoch 614 : Train loss -> 0.584251 Test loss -> 0.601565 Train acc -> 0.689966 Test acc -> 0.675097\n",
      "Epoch 615 : Train loss -> 0.584228 Test loss -> 0.601559 Train acc -> 0.689906 Test acc -> 0.675235\n",
      "Epoch 616 : Train loss -> 0.584206 Test loss -> 0.601552 Train acc -> 0.689906 Test acc -> 0.675373\n",
      "Epoch 617 : Train loss -> 0.584185 Test loss -> 0.601552 Train acc -> 0.689847 Test acc -> 0.675235\n",
      "Epoch 618 : Train loss -> 0.584163 Test loss -> 0.601552 Train acc -> 0.689788 Test acc -> 0.675235\n",
      "Epoch 619 : Train loss -> 0.584141 Test loss -> 0.601548 Train acc -> 0.689788 Test acc -> 0.675511\n",
      "Epoch 620 : Train loss -> 0.584119 Test loss -> 0.601538 Train acc -> 0.689788 Test acc -> 0.675511\n",
      "Epoch 621 : Train loss -> 0.584098 Test loss -> 0.601529 Train acc -> 0.689847 Test acc -> 0.675511\n",
      "Epoch 622 : Train loss -> 0.584077 Test loss -> 0.601524 Train acc -> 0.689966 Test acc -> 0.675511\n",
      "Epoch 623 : Train loss -> 0.584055 Test loss -> 0.601510 Train acc -> 0.690025 Test acc -> 0.675650\n",
      "Epoch 624 : Train loss -> 0.584034 Test loss -> 0.601497 Train acc -> 0.690084 Test acc -> 0.675788\n",
      "Epoch 625 : Train loss -> 0.584013 Test loss -> 0.601486 Train acc -> 0.689966 Test acc -> 0.675788\n",
      "Epoch 626 : Train loss -> 0.583992 Test loss -> 0.601472 Train acc -> 0.689966 Test acc -> 0.675650\n",
      "Epoch 627 : Train loss -> 0.583971 Test loss -> 0.601485 Train acc -> 0.689966 Test acc -> 0.675926\n",
      "Epoch 628 : Train loss -> 0.583949 Test loss -> 0.601474 Train acc -> 0.690025 Test acc -> 0.676202\n",
      "Epoch 629 : Train loss -> 0.583928 Test loss -> 0.601461 Train acc -> 0.690025 Test acc -> 0.675788\n",
      "Epoch 630 : Train loss -> 0.583908 Test loss -> 0.601465 Train acc -> 0.689966 Test acc -> 0.675788\n",
      "Epoch 631 : Train loss -> 0.583886 Test loss -> 0.601469 Train acc -> 0.689906 Test acc -> 0.676064\n",
      "Epoch 632 : Train loss -> 0.583866 Test loss -> 0.601446 Train acc -> 0.690084 Test acc -> 0.676202\n",
      "Epoch 633 : Train loss -> 0.583845 Test loss -> 0.601450 Train acc -> 0.689847 Test acc -> 0.675926\n",
      "Epoch 634 : Train loss -> 0.583825 Test loss -> 0.601433 Train acc -> 0.689788 Test acc -> 0.676064\n",
      "Epoch 635 : Train loss -> 0.583804 Test loss -> 0.601433 Train acc -> 0.689847 Test acc -> 0.676064\n",
      "Epoch 636 : Train loss -> 0.583783 Test loss -> 0.601426 Train acc -> 0.689847 Test acc -> 0.676064\n",
      "Epoch 637 : Train loss -> 0.583763 Test loss -> 0.601423 Train acc -> 0.690025 Test acc -> 0.676202\n",
      "Epoch 638 : Train loss -> 0.583743 Test loss -> 0.601404 Train acc -> 0.689966 Test acc -> 0.675788\n",
      "Epoch 639 : Train loss -> 0.583722 Test loss -> 0.601396 Train acc -> 0.690084 Test acc -> 0.675926\n",
      "Epoch 640 : Train loss -> 0.583702 Test loss -> 0.601409 Train acc -> 0.689906 Test acc -> 0.675788\n",
      "Epoch 641 : Train loss -> 0.583681 Test loss -> 0.601386 Train acc -> 0.689906 Test acc -> 0.675650\n",
      "Epoch 642 : Train loss -> 0.583660 Test loss -> 0.601370 Train acc -> 0.690143 Test acc -> 0.675788\n",
      "Epoch 643 : Train loss -> 0.583640 Test loss -> 0.601366 Train acc -> 0.690203 Test acc -> 0.675788\n",
      "Epoch 644 : Train loss -> 0.583619 Test loss -> 0.601364 Train acc -> 0.690025 Test acc -> 0.675650\n",
      "Epoch 645 : Train loss -> 0.583599 Test loss -> 0.601348 Train acc -> 0.690203 Test acc -> 0.675788\n",
      "Epoch 646 : Train loss -> 0.583579 Test loss -> 0.601333 Train acc -> 0.690262 Test acc -> 0.675788\n",
      "Epoch 647 : Train loss -> 0.583558 Test loss -> 0.601323 Train acc -> 0.690143 Test acc -> 0.675650\n",
      "Epoch 648 : Train loss -> 0.583538 Test loss -> 0.601319 Train acc -> 0.690380 Test acc -> 0.675650\n",
      "Epoch 649 : Train loss -> 0.583518 Test loss -> 0.601322 Train acc -> 0.690143 Test acc -> 0.675788\n",
      "Epoch 650 : Train loss -> 0.583498 Test loss -> 0.601311 Train acc -> 0.690143 Test acc -> 0.675788\n",
      "Epoch 651 : Train loss -> 0.583478 Test loss -> 0.601313 Train acc -> 0.689966 Test acc -> 0.675788\n",
      "Epoch 652 : Train loss -> 0.583458 Test loss -> 0.601287 Train acc -> 0.690084 Test acc -> 0.675788\n",
      "Epoch 653 : Train loss -> 0.583438 Test loss -> 0.601285 Train acc -> 0.690143 Test acc -> 0.675650\n",
      "Epoch 654 : Train loss -> 0.583418 Test loss -> 0.601278 Train acc -> 0.690203 Test acc -> 0.675650\n",
      "Epoch 655 : Train loss -> 0.583398 Test loss -> 0.601271 Train acc -> 0.690084 Test acc -> 0.675650\n",
      "Epoch 656 : Train loss -> 0.583378 Test loss -> 0.601262 Train acc -> 0.690025 Test acc -> 0.675511\n",
      "Epoch 657 : Train loss -> 0.583358 Test loss -> 0.601258 Train acc -> 0.690321 Test acc -> 0.675511\n",
      "Epoch 658 : Train loss -> 0.583338 Test loss -> 0.601255 Train acc -> 0.690262 Test acc -> 0.675511\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 659 : Train loss -> 0.583318 Test loss -> 0.601253 Train acc -> 0.689966 Test acc -> 0.675926\n",
      "Epoch 660 : Train loss -> 0.583298 Test loss -> 0.601236 Train acc -> 0.690084 Test acc -> 0.676064\n",
      "Epoch 661 : Train loss -> 0.583278 Test loss -> 0.601240 Train acc -> 0.690321 Test acc -> 0.676064\n",
      "Epoch 662 : Train loss -> 0.583258 Test loss -> 0.601222 Train acc -> 0.690321 Test acc -> 0.676064\n",
      "Epoch 663 : Train loss -> 0.583239 Test loss -> 0.601211 Train acc -> 0.690262 Test acc -> 0.676202\n",
      "Epoch 664 : Train loss -> 0.583218 Test loss -> 0.601215 Train acc -> 0.690440 Test acc -> 0.675926\n",
      "Epoch 665 : Train loss -> 0.583198 Test loss -> 0.601213 Train acc -> 0.690380 Test acc -> 0.675788\n",
      "Epoch 666 : Train loss -> 0.583178 Test loss -> 0.601183 Train acc -> 0.690262 Test acc -> 0.675926\n",
      "Epoch 667 : Train loss -> 0.583158 Test loss -> 0.601198 Train acc -> 0.690440 Test acc -> 0.675926\n",
      "Epoch 668 : Train loss -> 0.583139 Test loss -> 0.601188 Train acc -> 0.690440 Test acc -> 0.675926\n",
      "Epoch 669 : Train loss -> 0.583120 Test loss -> 0.601196 Train acc -> 0.690558 Test acc -> 0.675788\n",
      "Epoch 670 : Train loss -> 0.583100 Test loss -> 0.601184 Train acc -> 0.690380 Test acc -> 0.675788\n",
      "Epoch 671 : Train loss -> 0.583080 Test loss -> 0.601188 Train acc -> 0.690321 Test acc -> 0.675511\n",
      "Epoch 672 : Train loss -> 0.583061 Test loss -> 0.601176 Train acc -> 0.690558 Test acc -> 0.675926\n",
      "Epoch 673 : Train loss -> 0.583042 Test loss -> 0.601168 Train acc -> 0.690321 Test acc -> 0.675788\n",
      "Epoch 674 : Train loss -> 0.583022 Test loss -> 0.601174 Train acc -> 0.690380 Test acc -> 0.675650\n",
      "Epoch 675 : Train loss -> 0.583003 Test loss -> 0.601161 Train acc -> 0.690262 Test acc -> 0.675788\n",
      "Epoch 676 : Train loss -> 0.582984 Test loss -> 0.601153 Train acc -> 0.690262 Test acc -> 0.675788\n",
      "Epoch 677 : Train loss -> 0.582964 Test loss -> 0.601152 Train acc -> 0.690380 Test acc -> 0.676064\n",
      "Epoch 678 : Train loss -> 0.582945 Test loss -> 0.601147 Train acc -> 0.690321 Test acc -> 0.676202\n",
      "Epoch 679 : Train loss -> 0.582926 Test loss -> 0.601150 Train acc -> 0.690321 Test acc -> 0.676064\n",
      "Epoch 680 : Train loss -> 0.582906 Test loss -> 0.601140 Train acc -> 0.690321 Test acc -> 0.676064\n",
      "Epoch 681 : Train loss -> 0.582888 Test loss -> 0.601121 Train acc -> 0.690380 Test acc -> 0.676064\n",
      "Epoch 682 : Train loss -> 0.582869 Test loss -> 0.601132 Train acc -> 0.690143 Test acc -> 0.675926\n",
      "Epoch 683 : Train loss -> 0.582850 Test loss -> 0.601122 Train acc -> 0.690203 Test acc -> 0.675926\n",
      "Epoch 684 : Train loss -> 0.582831 Test loss -> 0.601114 Train acc -> 0.690203 Test acc -> 0.675788\n",
      "Epoch 685 : Train loss -> 0.582812 Test loss -> 0.601107 Train acc -> 0.690203 Test acc -> 0.675788\n",
      "Epoch 686 : Train loss -> 0.582793 Test loss -> 0.601103 Train acc -> 0.690262 Test acc -> 0.675650\n",
      "Epoch 687 : Train loss -> 0.582775 Test loss -> 0.601104 Train acc -> 0.690143 Test acc -> 0.675511\n",
      "Epoch 688 : Train loss -> 0.582756 Test loss -> 0.601097 Train acc -> 0.690203 Test acc -> 0.675788\n",
      "Epoch 689 : Train loss -> 0.582737 Test loss -> 0.601080 Train acc -> 0.690203 Test acc -> 0.675511\n",
      "Epoch 690 : Train loss -> 0.582719 Test loss -> 0.601078 Train acc -> 0.690203 Test acc -> 0.675650\n",
      "Epoch 691 : Train loss -> 0.582702 Test loss -> 0.601060 Train acc -> 0.690321 Test acc -> 0.675650\n",
      "Epoch 692 : Train loss -> 0.582683 Test loss -> 0.601058 Train acc -> 0.690321 Test acc -> 0.675373\n",
      "Epoch 693 : Train loss -> 0.582664 Test loss -> 0.601057 Train acc -> 0.690084 Test acc -> 0.675511\n",
      "Epoch 694 : Train loss -> 0.582646 Test loss -> 0.601052 Train acc -> 0.690262 Test acc -> 0.675511\n",
      "Epoch 695 : Train loss -> 0.582628 Test loss -> 0.601048 Train acc -> 0.690321 Test acc -> 0.675373\n",
      "Epoch 696 : Train loss -> 0.582609 Test loss -> 0.601039 Train acc -> 0.690321 Test acc -> 0.675373\n",
      "Epoch 697 : Train loss -> 0.582591 Test loss -> 0.601030 Train acc -> 0.690380 Test acc -> 0.675373\n",
      "Epoch 698 : Train loss -> 0.582572 Test loss -> 0.601032 Train acc -> 0.690380 Test acc -> 0.675373\n",
      "Epoch 699 : Train loss -> 0.582553 Test loss -> 0.601032 Train acc -> 0.690499 Test acc -> 0.675511\n",
      "Epoch 700 : Train loss -> 0.582535 Test loss -> 0.601016 Train acc -> 0.690440 Test acc -> 0.675511\n",
      "Epoch 701 : Train loss -> 0.582516 Test loss -> 0.601001 Train acc -> 0.690321 Test acc -> 0.675097\n",
      "Epoch 702 : Train loss -> 0.582498 Test loss -> 0.600994 Train acc -> 0.690380 Test acc -> 0.674959\n",
      "Epoch 703 : Train loss -> 0.582480 Test loss -> 0.601013 Train acc -> 0.690499 Test acc -> 0.675511\n",
      "Epoch 704 : Train loss -> 0.582461 Test loss -> 0.600996 Train acc -> 0.690380 Test acc -> 0.675235\n",
      "Epoch 705 : Train loss -> 0.582444 Test loss -> 0.600978 Train acc -> 0.690617 Test acc -> 0.675097\n",
      "Epoch 706 : Train loss -> 0.582425 Test loss -> 0.600997 Train acc -> 0.690676 Test acc -> 0.675235\n",
      "Epoch 707 : Train loss -> 0.582407 Test loss -> 0.600972 Train acc -> 0.690499 Test acc -> 0.674959\n",
      "Epoch 708 : Train loss -> 0.582389 Test loss -> 0.600974 Train acc -> 0.690854 Test acc -> 0.675373\n",
      "Epoch 709 : Train loss -> 0.582371 Test loss -> 0.600957 Train acc -> 0.690676 Test acc -> 0.675373\n",
      "Epoch 710 : Train loss -> 0.582353 Test loss -> 0.600963 Train acc -> 0.690558 Test acc -> 0.675097\n",
      "Epoch 711 : Train loss -> 0.582335 Test loss -> 0.600971 Train acc -> 0.691091 Test acc -> 0.674959\n",
      "Epoch 712 : Train loss -> 0.582317 Test loss -> 0.600949 Train acc -> 0.690973 Test acc -> 0.674959\n",
      "Epoch 713 : Train loss -> 0.582299 Test loss -> 0.600948 Train acc -> 0.690913 Test acc -> 0.674820\n",
      "Epoch 714 : Train loss -> 0.582282 Test loss -> 0.600939 Train acc -> 0.691032 Test acc -> 0.674820\n",
      "Epoch 715 : Train loss -> 0.582264 Test loss -> 0.600939 Train acc -> 0.691091 Test acc -> 0.674820\n",
      "Epoch 716 : Train loss -> 0.582246 Test loss -> 0.600940 Train acc -> 0.691150 Test acc -> 0.674820\n",
      "Epoch 717 : Train loss -> 0.582228 Test loss -> 0.600928 Train acc -> 0.691091 Test acc -> 0.674820\n",
      "Epoch 718 : Train loss -> 0.582211 Test loss -> 0.600912 Train acc -> 0.691269 Test acc -> 0.674820\n",
      "Epoch 719 : Train loss -> 0.582192 Test loss -> 0.600914 Train acc -> 0.691150 Test acc -> 0.675097\n",
      "Epoch 720 : Train loss -> 0.582175 Test loss -> 0.600905 Train acc -> 0.691387 Test acc -> 0.674959\n",
      "Epoch 721 : Train loss -> 0.582157 Test loss -> 0.600901 Train acc -> 0.691269 Test acc -> 0.674959\n",
      "Epoch 722 : Train loss -> 0.582139 Test loss -> 0.600895 Train acc -> 0.691269 Test acc -> 0.674959\n",
      "Epoch 723 : Train loss -> 0.582122 Test loss -> 0.600903 Train acc -> 0.691210 Test acc -> 0.674959\n",
      "Epoch 724 : Train loss -> 0.582104 Test loss -> 0.600893 Train acc -> 0.691387 Test acc -> 0.674820\n",
      "Epoch 725 : Train loss -> 0.582087 Test loss -> 0.600891 Train acc -> 0.691328 Test acc -> 0.674820\n",
      "Epoch 726 : Train loss -> 0.582070 Test loss -> 0.600892 Train acc -> 0.691328 Test acc -> 0.674959\n",
      "Epoch 727 : Train loss -> 0.582052 Test loss -> 0.600894 Train acc -> 0.691091 Test acc -> 0.674959\n",
      "Epoch 728 : Train loss -> 0.582035 Test loss -> 0.600875 Train acc -> 0.691210 Test acc -> 0.674959\n",
      "Epoch 729 : Train loss -> 0.582018 Test loss -> 0.600880 Train acc -> 0.691150 Test acc -> 0.674820\n",
      "Epoch 730 : Train loss -> 0.582000 Test loss -> 0.600859 Train acc -> 0.691269 Test acc -> 0.674820\n",
      "Epoch 731 : Train loss -> 0.581983 Test loss -> 0.600869 Train acc -> 0.691032 Test acc -> 0.674959\n",
      "Epoch 732 : Train loss -> 0.581966 Test loss -> 0.600876 Train acc -> 0.691150 Test acc -> 0.674959\n",
      "Epoch 733 : Train loss -> 0.581949 Test loss -> 0.600863 Train acc -> 0.691150 Test acc -> 0.674820\n",
      "Epoch 734 : Train loss -> 0.581931 Test loss -> 0.600857 Train acc -> 0.691150 Test acc -> 0.674959\n",
      "Epoch 735 : Train loss -> 0.581915 Test loss -> 0.600856 Train acc -> 0.691210 Test acc -> 0.674959\n",
      "Epoch 736 : Train loss -> 0.581897 Test loss -> 0.600841 Train acc -> 0.691269 Test acc -> 0.674959\n",
      "Epoch 737 : Train loss -> 0.581880 Test loss -> 0.600834 Train acc -> 0.691328 Test acc -> 0.674959\n",
      "Epoch 738 : Train loss -> 0.581863 Test loss -> 0.600824 Train acc -> 0.691150 Test acc -> 0.675235\n",
      "Epoch 739 : Train loss -> 0.581847 Test loss -> 0.600840 Train acc -> 0.691210 Test acc -> 0.674820\n",
      "Epoch 740 : Train loss -> 0.581829 Test loss -> 0.600830 Train acc -> 0.691210 Test acc -> 0.674682\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 741 : Train loss -> 0.581813 Test loss -> 0.600826 Train acc -> 0.691269 Test acc -> 0.674820\n",
      "Epoch 742 : Train loss -> 0.581796 Test loss -> 0.600833 Train acc -> 0.691269 Test acc -> 0.674820\n",
      "Epoch 743 : Train loss -> 0.581780 Test loss -> 0.600811 Train acc -> 0.691269 Test acc -> 0.675097\n",
      "Epoch 744 : Train loss -> 0.581763 Test loss -> 0.600797 Train acc -> 0.691328 Test acc -> 0.674820\n",
      "Epoch 745 : Train loss -> 0.581747 Test loss -> 0.600797 Train acc -> 0.691269 Test acc -> 0.675097\n",
      "Epoch 746 : Train loss -> 0.581729 Test loss -> 0.600786 Train acc -> 0.691328 Test acc -> 0.674820\n",
      "Epoch 747 : Train loss -> 0.581713 Test loss -> 0.600781 Train acc -> 0.691328 Test acc -> 0.674959\n",
      "Epoch 748 : Train loss -> 0.581696 Test loss -> 0.600777 Train acc -> 0.691269 Test acc -> 0.675235\n",
      "Epoch 749 : Train loss -> 0.581680 Test loss -> 0.600789 Train acc -> 0.691387 Test acc -> 0.674959\n",
      "Epoch 750 : Train loss -> 0.581663 Test loss -> 0.600773 Train acc -> 0.691387 Test acc -> 0.674959\n",
      "Epoch 751 : Train loss -> 0.581647 Test loss -> 0.600772 Train acc -> 0.691269 Test acc -> 0.675097\n",
      "Epoch 752 : Train loss -> 0.581631 Test loss -> 0.600764 Train acc -> 0.691328 Test acc -> 0.675235\n",
      "Epoch 753 : Train loss -> 0.581614 Test loss -> 0.600770 Train acc -> 0.691269 Test acc -> 0.675235\n",
      "Epoch 754 : Train loss -> 0.581598 Test loss -> 0.600767 Train acc -> 0.691328 Test acc -> 0.674959\n",
      "Epoch 755 : Train loss -> 0.581582 Test loss -> 0.600763 Train acc -> 0.691328 Test acc -> 0.674820\n",
      "Epoch 756 : Train loss -> 0.581566 Test loss -> 0.600771 Train acc -> 0.691446 Test acc -> 0.674820\n",
      "Epoch 757 : Train loss -> 0.581549 Test loss -> 0.600766 Train acc -> 0.691506 Test acc -> 0.674820\n",
      "Epoch 758 : Train loss -> 0.581533 Test loss -> 0.600767 Train acc -> 0.691506 Test acc -> 0.674820\n",
      "Epoch 759 : Train loss -> 0.581517 Test loss -> 0.600751 Train acc -> 0.691446 Test acc -> 0.674682\n",
      "Epoch 760 : Train loss -> 0.581501 Test loss -> 0.600755 Train acc -> 0.691446 Test acc -> 0.674959\n",
      "Epoch 761 : Train loss -> 0.581484 Test loss -> 0.600748 Train acc -> 0.691387 Test acc -> 0.674682\n",
      "Epoch 762 : Train loss -> 0.581469 Test loss -> 0.600734 Train acc -> 0.691446 Test acc -> 0.674820\n",
      "Epoch 763 : Train loss -> 0.581453 Test loss -> 0.600724 Train acc -> 0.691328 Test acc -> 0.674959\n",
      "Epoch 764 : Train loss -> 0.581437 Test loss -> 0.600730 Train acc -> 0.691446 Test acc -> 0.674820\n",
      "Epoch 765 : Train loss -> 0.581421 Test loss -> 0.600739 Train acc -> 0.691269 Test acc -> 0.674820\n",
      "Epoch 766 : Train loss -> 0.581404 Test loss -> 0.600725 Train acc -> 0.691328 Test acc -> 0.674820\n",
      "Epoch 767 : Train loss -> 0.581389 Test loss -> 0.600725 Train acc -> 0.691269 Test acc -> 0.674959\n",
      "Epoch 768 : Train loss -> 0.581373 Test loss -> 0.600729 Train acc -> 0.691269 Test acc -> 0.674959\n",
      "Epoch 769 : Train loss -> 0.581357 Test loss -> 0.600730 Train acc -> 0.691446 Test acc -> 0.674820\n",
      "Epoch 770 : Train loss -> 0.581341 Test loss -> 0.600711 Train acc -> 0.691387 Test acc -> 0.674959\n"
     ]
    }
   ],
   "source": [
    "trainer.train(1000,256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainer.dataloader = DataLoader(trainer.train_data, trainer.train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_num = int(len(n_data)*0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_label = a[:train_num,:], r[:train_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data, test_label = a[train_num:,:], r[train_num:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((16882, 34), (7236, 34))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7236,), (16882,))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_label.shape, train_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

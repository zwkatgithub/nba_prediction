{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "\n",
    "'''通过方差的百分比来计算将数据降到多少维是比较合适的，\n",
    "函数传入的参数是特征值和百分比percentage，返回需要降到的维度数num'''\n",
    "def eigValPct(eigVals,percentage):\n",
    "    sortArray=sort(eigVals) #使用numpy中的sort()对特征值按照从小到大排序\n",
    "    sortArray=sortArray[-1::-1] #特征值从大到小排序\n",
    "    arraySum=sum(sortArray) #数据全部的方差arraySum\n",
    "    tempSum=0\n",
    "    num=0\n",
    "    for i in sortArray:\n",
    "        tempSum+=i\n",
    "        num+=1\n",
    "        if tempSum>=arraySum*percentage:\n",
    "            return num\n",
    "\n",
    "'''pca函数有两个参数，其中dataMat是已经转换成矩阵matrix形式的数据集，列表示特征；\n",
    "其中的percentage表示取前多少个特征需要达到的方差占比，默认为0.9'''\n",
    "def pca(dataMat,percentage=0.9):\n",
    "    meanVals=mean(dataMat,axis=0)  #对每一列求平均值，因为协方差的计算中需要减去均值\n",
    "    meanRemoved=dataMat-meanVals\n",
    "    covMat=cov(meanRemoved,rowvar=0)  #cov()计算方差\n",
    "    eigVals,eigVects=linalg.eig(mat(covMat))  #利用numpy中寻找特征值和特征向量的模块linalg中的eig()方法\n",
    "    k=eigValPct(eigVals,percentage) #要达到方差的百分比percentage，需要前k个向量\n",
    "    eigValInd=argsort(eigVals)  #对特征值eigVals从小到大排序\n",
    "    eigValInd=eigValInd[:-(k+1):-1] #从排好序的特征值，从后往前取k个，这样就实现了特征值的从大到小排列\n",
    "    redEigVects=eigVects[:,eigValInd]   #返回排序后特征值对应的特征向量redEigVects（主成分）\n",
    "    lowDDataMat=meanRemoved*redEigVects #将原始数据投影到主成分上得到新的低维数据lowDDataMat\n",
    "    reconMat=(lowDDataMat*redEigVects.T)+meanVals   #得到重构数据reconMat\n",
    "    return lowDDataMat,reconMat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/processed/data.csv','r') as f:\n",
    "    rawdata = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = rawdata.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawdata = [[float(d) for d in r.split(',')] for r in a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rawdata = np.array(rawdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24118, 64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawdata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = rawdata[:,:55]\n",
    "label = rawdata[:,55:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24118, 55)\n"
     ]
    }
   ],
   "source": [
    "n_data = normalize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a,b = pca(n_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24118, 34)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trainers import MLPTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mxnet import gluon, nd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainer = MLPTrainer('three_pt',gluon.loss.SoftmaxCrossEntropyLoss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainer.dataload(nd.array(train_data),train_label, nd.array(test_data),test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from write_data import OutputData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = label[:,OutputData.colName().index('three_pt')]*3+label[:,OutputData.colName().index('ft')] + label[:,OutputData.colName().index('in_pts')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r = nd.cast(nd.array(res > 0),'float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.initnet(0.00001,0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 : Train loss -> 0.869503 Test loss -> 0.877861 Train acc -> 0.498638 Test acc -> 0.495854\n",
      "Epoch 1 : Train loss -> 0.799384 Test loss -> 0.809927 Train acc -> 0.505272 Test acc -> 0.504008\n",
      "Epoch 2 : Train loss -> 0.754703 Test loss -> 0.767651 Train acc -> 0.518718 Test acc -> 0.516446\n",
      "Epoch 3 : Train loss -> 0.724194 Test loss -> 0.738444 Train acc -> 0.538976 Test acc -> 0.531233\n",
      "Epoch 4 : Train loss -> 0.701199 Test loss -> 0.716194 Train acc -> 0.560182 Test acc -> 0.551410\n",
      "Epoch 5 : Train loss -> 0.683469 Test loss -> 0.698491 Train acc -> 0.580618 Test acc -> 0.570343\n",
      "Epoch 6 : Train loss -> 0.669216 Test loss -> 0.683928 Train acc -> 0.600699 Test acc -> 0.584439\n",
      "Epoch 7 : Train loss -> 0.657401 Test loss -> 0.671539 Train acc -> 0.613434 Test acc -> 0.598673\n",
      "Epoch 8 : Train loss -> 0.647963 Test loss -> 0.661356 Train acc -> 0.626881 Test acc -> 0.608485\n",
      "Epoch 9 : Train loss -> 0.640067 Test loss -> 0.652794 Train acc -> 0.637247 Test acc -> 0.617745\n",
      "Epoch 10 : Train loss -> 0.633679 Test loss -> 0.645741 Train acc -> 0.644296 Test acc -> 0.628386\n",
      "Epoch 11 : Train loss -> 0.628324 Test loss -> 0.639921 Train acc -> 0.650219 Test acc -> 0.634190\n",
      "Epoch 12 : Train loss -> 0.624052 Test loss -> 0.635260 Train acc -> 0.655846 Test acc -> 0.639994\n",
      "Epoch 13 : Train loss -> 0.620489 Test loss -> 0.631454 Train acc -> 0.658927 Test acc -> 0.644140\n",
      "Epoch 14 : Train loss -> 0.617588 Test loss -> 0.628322 Train acc -> 0.662658 Test acc -> 0.645937\n",
      "Epoch 15 : Train loss -> 0.615109 Test loss -> 0.625725 Train acc -> 0.664732 Test acc -> 0.649254\n",
      "Epoch 16 : Train loss -> 0.613044 Test loss -> 0.623680 Train acc -> 0.666331 Test acc -> 0.653261\n",
      "Epoch 17 : Train loss -> 0.611295 Test loss -> 0.622024 Train acc -> 0.667930 Test acc -> 0.654505\n",
      "Epoch 18 : Train loss -> 0.609721 Test loss -> 0.620456 Train acc -> 0.669352 Test acc -> 0.656440\n",
      "Epoch 19 : Train loss -> 0.608414 Test loss -> 0.619253 Train acc -> 0.670596 Test acc -> 0.656716\n",
      "Epoch 20 : Train loss -> 0.607251 Test loss -> 0.618193 Train acc -> 0.671307 Test acc -> 0.658098\n",
      "Epoch 21 : Train loss -> 0.606203 Test loss -> 0.617278 Train acc -> 0.671958 Test acc -> 0.656855\n",
      "Epoch 22 : Train loss -> 0.605223 Test loss -> 0.616352 Train acc -> 0.673558 Test acc -> 0.658237\n",
      "Epoch 23 : Train loss -> 0.604344 Test loss -> 0.615721 Train acc -> 0.674387 Test acc -> 0.659342\n",
      "Epoch 24 : Train loss -> 0.603557 Test loss -> 0.614940 Train acc -> 0.674920 Test acc -> 0.660448\n",
      "Epoch 25 : Train loss -> 0.602769 Test loss -> 0.614325 Train acc -> 0.675868 Test acc -> 0.661553\n",
      "Epoch 26 : Train loss -> 0.602045 Test loss -> 0.613788 Train acc -> 0.675986 Test acc -> 0.662106\n",
      "Epoch 27 : Train loss -> 0.601362 Test loss -> 0.613135 Train acc -> 0.676697 Test acc -> 0.662935\n",
      "Epoch 28 : Train loss -> 0.600705 Test loss -> 0.612622 Train acc -> 0.677349 Test acc -> 0.664594\n",
      "Epoch 29 : Train loss -> 0.600099 Test loss -> 0.612046 Train acc -> 0.677763 Test acc -> 0.666667\n",
      "Epoch 30 : Train loss -> 0.599509 Test loss -> 0.611706 Train acc -> 0.678711 Test acc -> 0.666805\n",
      "Epoch 31 : Train loss -> 0.598933 Test loss -> 0.611187 Train acc -> 0.679066 Test acc -> 0.666805\n",
      "Epoch 32 : Train loss -> 0.598388 Test loss -> 0.610772 Train acc -> 0.679422 Test acc -> 0.667081\n",
      "Epoch 33 : Train loss -> 0.597878 Test loss -> 0.610462 Train acc -> 0.679540 Test acc -> 0.668463\n",
      "Epoch 34 : Train loss -> 0.597372 Test loss -> 0.610000 Train acc -> 0.679659 Test acc -> 0.669016\n",
      "Epoch 35 : Train loss -> 0.596903 Test loss -> 0.609569 Train acc -> 0.680192 Test acc -> 0.668049\n",
      "Epoch 36 : Train loss -> 0.596435 Test loss -> 0.609269 Train acc -> 0.679896 Test acc -> 0.669431\n",
      "Epoch 37 : Train loss -> 0.595976 Test loss -> 0.608918 Train acc -> 0.680073 Test acc -> 0.669292\n",
      "Epoch 38 : Train loss -> 0.595534 Test loss -> 0.608610 Train acc -> 0.680488 Test acc -> 0.669569\n",
      "Epoch 39 : Train loss -> 0.595123 Test loss -> 0.608290 Train acc -> 0.680725 Test acc -> 0.670398\n",
      "Epoch 40 : Train loss -> 0.594714 Test loss -> 0.607969 Train acc -> 0.681199 Test acc -> 0.670951\n",
      "Epoch 41 : Train loss -> 0.594307 Test loss -> 0.607618 Train acc -> 0.681199 Test acc -> 0.671504\n",
      "Epoch 42 : Train loss -> 0.593926 Test loss -> 0.607297 Train acc -> 0.681258 Test acc -> 0.671365\n",
      "Epoch 43 : Train loss -> 0.593555 Test loss -> 0.607146 Train acc -> 0.681495 Test acc -> 0.671642\n",
      "Epoch 44 : Train loss -> 0.593194 Test loss -> 0.606838 Train acc -> 0.681910 Test acc -> 0.672333\n",
      "Epoch 45 : Train loss -> 0.592846 Test loss -> 0.606636 Train acc -> 0.682680 Test acc -> 0.673300\n",
      "Epoch 46 : Train loss -> 0.592511 Test loss -> 0.606403 Train acc -> 0.682798 Test acc -> 0.673300\n",
      "Epoch 47 : Train loss -> 0.592164 Test loss -> 0.606171 Train acc -> 0.683094 Test acc -> 0.671780\n",
      "Epoch 48 : Train loss -> 0.591838 Test loss -> 0.606024 Train acc -> 0.683331 Test acc -> 0.671780\n",
      "Epoch 49 : Train loss -> 0.591519 Test loss -> 0.605731 Train acc -> 0.683805 Test acc -> 0.671504\n",
      "Epoch 50 : Train loss -> 0.591212 Test loss -> 0.605533 Train acc -> 0.683805 Test acc -> 0.671365\n",
      "Epoch 51 : Train loss -> 0.590909 Test loss -> 0.605304 Train acc -> 0.684161 Test acc -> 0.671504\n",
      "Epoch 52 : Train loss -> 0.590613 Test loss -> 0.605196 Train acc -> 0.684101 Test acc -> 0.671780\n",
      "Epoch 53 : Train loss -> 0.590345 Test loss -> 0.605063 Train acc -> 0.684457 Test acc -> 0.671780\n",
      "Epoch 54 : Train loss -> 0.590065 Test loss -> 0.604729 Train acc -> 0.684635 Test acc -> 0.671780\n",
      "Epoch 55 : Train loss -> 0.589797 Test loss -> 0.604663 Train acc -> 0.684398 Test acc -> 0.672609\n",
      "Epoch 56 : Train loss -> 0.589533 Test loss -> 0.604441 Train acc -> 0.685168 Test acc -> 0.673438\n",
      "Epoch 57 : Train loss -> 0.589277 Test loss -> 0.604328 Train acc -> 0.685227 Test acc -> 0.672886\n",
      "Epoch 58 : Train loss -> 0.589035 Test loss -> 0.604131 Train acc -> 0.685582 Test acc -> 0.673300\n",
      "Epoch 59 : Train loss -> 0.588788 Test loss -> 0.604054 Train acc -> 0.685760 Test acc -> 0.673438\n",
      "Epoch 60 : Train loss -> 0.588550 Test loss -> 0.603758 Train acc -> 0.686115 Test acc -> 0.674406\n",
      "Epoch 61 : Train loss -> 0.588315 Test loss -> 0.603657 Train acc -> 0.685997 Test acc -> 0.673715\n",
      "Epoch 62 : Train loss -> 0.588089 Test loss -> 0.603528 Train acc -> 0.686115 Test acc -> 0.674129\n",
      "Epoch 63 : Train loss -> 0.587871 Test loss -> 0.603363 Train acc -> 0.686471 Test acc -> 0.673577\n",
      "Epoch 64 : Train loss -> 0.587640 Test loss -> 0.603294 Train acc -> 0.686945 Test acc -> 0.673853\n",
      "Epoch 65 : Train loss -> 0.587433 Test loss -> 0.603257 Train acc -> 0.686530 Test acc -> 0.673438\n",
      "Epoch 66 : Train loss -> 0.587214 Test loss -> 0.603075 Train acc -> 0.686767 Test acc -> 0.673577\n",
      "Epoch 67 : Train loss -> 0.587010 Test loss -> 0.602913 Train acc -> 0.686945 Test acc -> 0.673300\n",
      "Epoch 68 : Train loss -> 0.586803 Test loss -> 0.602865 Train acc -> 0.687537 Test acc -> 0.673577\n",
      "Epoch 69 : Train loss -> 0.586600 Test loss -> 0.602806 Train acc -> 0.687833 Test acc -> 0.673991\n",
      "Epoch 70 : Train loss -> 0.586395 Test loss -> 0.602670 Train acc -> 0.688011 Test acc -> 0.673991\n",
      "Epoch 71 : Train loss -> 0.586202 Test loss -> 0.602548 Train acc -> 0.687952 Test acc -> 0.674406\n",
      "Epoch 72 : Train loss -> 0.586010 Test loss -> 0.602464 Train acc -> 0.688307 Test acc -> 0.674129\n",
      "Epoch 73 : Train loss -> 0.585823 Test loss -> 0.602248 Train acc -> 0.688544 Test acc -> 0.674268\n",
      "Epoch 74 : Train loss -> 0.585634 Test loss -> 0.602199 Train acc -> 0.688544 Test acc -> 0.674268\n",
      "Epoch 75 : Train loss -> 0.585451 Test loss -> 0.602057 Train acc -> 0.688307 Test acc -> 0.673991\n",
      "Epoch 76 : Train loss -> 0.585274 Test loss -> 0.602036 Train acc -> 0.688544 Test acc -> 0.674544\n",
      "Epoch 77 : Train loss -> 0.585104 Test loss -> 0.602074 Train acc -> 0.688662 Test acc -> 0.675097\n",
      "Epoch 78 : Train loss -> 0.584924 Test loss -> 0.601950 Train acc -> 0.688722 Test acc -> 0.675235\n",
      "Epoch 79 : Train loss -> 0.584763 Test loss -> 0.601923 Train acc -> 0.689018 Test acc -> 0.674682\n",
      "Epoch 80 : Train loss -> 0.584596 Test loss -> 0.601779 Train acc -> 0.689196 Test acc -> 0.674544\n",
      "Epoch 81 : Train loss -> 0.584427 Test loss -> 0.601682 Train acc -> 0.689018 Test acc -> 0.674544\n",
      "Epoch 82 : Train loss -> 0.584262 Test loss -> 0.601654 Train acc -> 0.688899 Test acc -> 0.674268\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83 : Train loss -> 0.584108 Test loss -> 0.601573 Train acc -> 0.689196 Test acc -> 0.673853\n",
      "Epoch 84 : Train loss -> 0.583945 Test loss -> 0.601453 Train acc -> 0.689788 Test acc -> 0.673991\n",
      "Epoch 85 : Train loss -> 0.583795 Test loss -> 0.601445 Train acc -> 0.689669 Test acc -> 0.673715\n",
      "Epoch 86 : Train loss -> 0.583645 Test loss -> 0.601362 Train acc -> 0.689788 Test acc -> 0.673853\n",
      "Epoch 87 : Train loss -> 0.583492 Test loss -> 0.601337 Train acc -> 0.689788 Test acc -> 0.673577\n",
      "Epoch 88 : Train loss -> 0.583348 Test loss -> 0.601217 Train acc -> 0.689610 Test acc -> 0.673853\n",
      "Epoch 89 : Train loss -> 0.583201 Test loss -> 0.601198 Train acc -> 0.689847 Test acc -> 0.674406\n",
      "Epoch 90 : Train loss -> 0.583059 Test loss -> 0.601183 Train acc -> 0.690143 Test acc -> 0.674129\n",
      "Epoch 91 : Train loss -> 0.582916 Test loss -> 0.601050 Train acc -> 0.690736 Test acc -> 0.673991\n",
      "Epoch 92 : Train loss -> 0.582780 Test loss -> 0.601055 Train acc -> 0.690617 Test acc -> 0.674406\n",
      "Epoch 93 : Train loss -> 0.582636 Test loss -> 0.601035 Train acc -> 0.691150 Test acc -> 0.674406\n",
      "Epoch 94 : Train loss -> 0.582503 Test loss -> 0.600836 Train acc -> 0.691506 Test acc -> 0.673991\n",
      "Epoch 95 : Train loss -> 0.582355 Test loss -> 0.600792 Train acc -> 0.691210 Test acc -> 0.674959\n",
      "Epoch 96 : Train loss -> 0.582216 Test loss -> 0.600850 Train acc -> 0.691446 Test acc -> 0.674820\n",
      "Epoch 97 : Train loss -> 0.582083 Test loss -> 0.600815 Train acc -> 0.691683 Test acc -> 0.674959\n",
      "Epoch 98 : Train loss -> 0.581941 Test loss -> 0.600757 Train acc -> 0.691683 Test acc -> 0.674682\n",
      "Epoch 99 : Train loss -> 0.581804 Test loss -> 0.600719 Train acc -> 0.691506 Test acc -> 0.675097\n",
      "Epoch 100 : Train loss -> 0.581658 Test loss -> 0.600668 Train acc -> 0.691861 Test acc -> 0.674959\n",
      "Epoch 101 : Train loss -> 0.581524 Test loss -> 0.600674 Train acc -> 0.691802 Test acc -> 0.674959\n",
      "Epoch 102 : Train loss -> 0.581386 Test loss -> 0.600641 Train acc -> 0.692276 Test acc -> 0.675788\n",
      "Epoch 103 : Train loss -> 0.581253 Test loss -> 0.600612 Train acc -> 0.692454 Test acc -> 0.675926\n",
      "Epoch 104 : Train loss -> 0.581119 Test loss -> 0.600554 Train acc -> 0.692513 Test acc -> 0.675511\n",
      "Epoch 105 : Train loss -> 0.580980 Test loss -> 0.600542 Train acc -> 0.692809 Test acc -> 0.675373\n",
      "Epoch 106 : Train loss -> 0.580839 Test loss -> 0.600495 Train acc -> 0.692572 Test acc -> 0.676202\n",
      "Epoch 107 : Train loss -> 0.580697 Test loss -> 0.600456 Train acc -> 0.692987 Test acc -> 0.675511\n",
      "Epoch 108 : Train loss -> 0.580558 Test loss -> 0.600472 Train acc -> 0.692572 Test acc -> 0.676341\n",
      "Epoch 109 : Train loss -> 0.580422 Test loss -> 0.600356 Train acc -> 0.692631 Test acc -> 0.676341\n",
      "Epoch 110 : Train loss -> 0.580286 Test loss -> 0.600371 Train acc -> 0.692987 Test acc -> 0.676341\n",
      "Epoch 111 : Train loss -> 0.580151 Test loss -> 0.600426 Train acc -> 0.692987 Test acc -> 0.676893\n",
      "Epoch 112 : Train loss -> 0.580015 Test loss -> 0.600336 Train acc -> 0.692927 Test acc -> 0.676202\n",
      "Epoch 113 : Train loss -> 0.579878 Test loss -> 0.600454 Train acc -> 0.693105 Test acc -> 0.676479\n",
      "Epoch 114 : Train loss -> 0.579743 Test loss -> 0.600378 Train acc -> 0.693283 Test acc -> 0.676202\n",
      "Epoch 115 : Train loss -> 0.579624 Test loss -> 0.600286 Train acc -> 0.693164 Test acc -> 0.675511\n",
      "Epoch 116 : Train loss -> 0.579477 Test loss -> 0.600395 Train acc -> 0.693342 Test acc -> 0.675650\n",
      "Epoch 117 : Train loss -> 0.579346 Test loss -> 0.600377 Train acc -> 0.693697 Test acc -> 0.675373\n",
      "Epoch 118 : Train loss -> 0.579210 Test loss -> 0.600187 Train acc -> 0.693934 Test acc -> 0.675650\n",
      "Epoch 119 : Train loss -> 0.579088 Test loss -> 0.600244 Train acc -> 0.694112 Test acc -> 0.675650\n",
      "Epoch 120 : Train loss -> 0.578947 Test loss -> 0.600236 Train acc -> 0.694231 Test acc -> 0.675235\n",
      "Epoch 121 : Train loss -> 0.578812 Test loss -> 0.600139 Train acc -> 0.694349 Test acc -> 0.675650\n",
      "Epoch 122 : Train loss -> 0.578683 Test loss -> 0.600181 Train acc -> 0.694349 Test acc -> 0.676341\n",
      "Epoch 123 : Train loss -> 0.578552 Test loss -> 0.600156 Train acc -> 0.694586 Test acc -> 0.675650\n",
      "Epoch 124 : Train loss -> 0.578423 Test loss -> 0.600186 Train acc -> 0.695238 Test acc -> 0.675097\n",
      "Epoch 125 : Train loss -> 0.578287 Test loss -> 0.600044 Train acc -> 0.694349 Test acc -> 0.675097\n",
      "Epoch 126 : Train loss -> 0.578156 Test loss -> 0.600110 Train acc -> 0.695178 Test acc -> 0.675373\n",
      "Epoch 127 : Train loss -> 0.578022 Test loss -> 0.600054 Train acc -> 0.695001 Test acc -> 0.675511\n",
      "Epoch 128 : Train loss -> 0.577902 Test loss -> 0.600061 Train acc -> 0.694645 Test acc -> 0.675650\n",
      "Epoch 129 : Train loss -> 0.577771 Test loss -> 0.600075 Train acc -> 0.694941 Test acc -> 0.675650\n",
      "Epoch 130 : Train loss -> 0.577642 Test loss -> 0.600006 Train acc -> 0.694882 Test acc -> 0.675511\n",
      "Epoch 131 : Train loss -> 0.577516 Test loss -> 0.600053 Train acc -> 0.695060 Test acc -> 0.676064\n",
      "Epoch 132 : Train loss -> 0.577381 Test loss -> 0.600008 Train acc -> 0.695297 Test acc -> 0.676479\n",
      "Epoch 133 : Train loss -> 0.577259 Test loss -> 0.600005 Train acc -> 0.695297 Test acc -> 0.676202\n",
      "Epoch 134 : Train loss -> 0.577141 Test loss -> 0.599972 Train acc -> 0.695474 Test acc -> 0.676202\n",
      "Epoch 135 : Train loss -> 0.577013 Test loss -> 0.600006 Train acc -> 0.695534 Test acc -> 0.676202\n",
      "Epoch 136 : Train loss -> 0.576889 Test loss -> 0.599892 Train acc -> 0.695771 Test acc -> 0.676341\n",
      "Epoch 137 : Train loss -> 0.576764 Test loss -> 0.600073 Train acc -> 0.696422 Test acc -> 0.676064\n",
      "Epoch 138 : Train loss -> 0.576647 Test loss -> 0.600092 Train acc -> 0.695948 Test acc -> 0.675788\n",
      "Epoch 139 : Train loss -> 0.576522 Test loss -> 0.600014 Train acc -> 0.696541 Test acc -> 0.674959\n",
      "Epoch 140 : Train loss -> 0.576400 Test loss -> 0.599954 Train acc -> 0.696245 Test acc -> 0.674959\n",
      "Epoch 141 : Train loss -> 0.576277 Test loss -> 0.600066 Train acc -> 0.696481 Test acc -> 0.675235\n",
      "Epoch 142 : Train loss -> 0.576157 Test loss -> 0.600062 Train acc -> 0.695771 Test acc -> 0.674959\n",
      "Epoch 143 : Train loss -> 0.576041 Test loss -> 0.600053 Train acc -> 0.696422 Test acc -> 0.674820\n",
      "Epoch 144 : Train loss -> 0.575922 Test loss -> 0.600069 Train acc -> 0.696659 Test acc -> 0.674820\n",
      "Epoch 145 : Train loss -> 0.575816 Test loss -> 0.600041 Train acc -> 0.695830 Test acc -> 0.674544\n",
      "Epoch 146 : Train loss -> 0.575691 Test loss -> 0.600111 Train acc -> 0.696067 Test acc -> 0.674820\n",
      "Epoch 147 : Train loss -> 0.575580 Test loss -> 0.599956 Train acc -> 0.696541 Test acc -> 0.674268\n",
      "Epoch 148 : Train loss -> 0.575458 Test loss -> 0.600114 Train acc -> 0.696008 Test acc -> 0.675097\n",
      "Epoch 149 : Train loss -> 0.575348 Test loss -> 0.599925 Train acc -> 0.696600 Test acc -> 0.674268\n",
      "Epoch 150 : Train loss -> 0.575233 Test loss -> 0.600175 Train acc -> 0.697192 Test acc -> 0.674544\n",
      "Epoch 151 : Train loss -> 0.575111 Test loss -> 0.600140 Train acc -> 0.696778 Test acc -> 0.673991\n",
      "Epoch 152 : Train loss -> 0.575000 Test loss -> 0.600133 Train acc -> 0.697133 Test acc -> 0.674129\n",
      "Epoch 153 : Train loss -> 0.574879 Test loss -> 0.600131 Train acc -> 0.696659 Test acc -> 0.675235\n",
      "Epoch 154 : Train loss -> 0.574767 Test loss -> 0.600086 Train acc -> 0.696600 Test acc -> 0.674129\n",
      "Epoch 155 : Train loss -> 0.574646 Test loss -> 0.600152 Train acc -> 0.696659 Test acc -> 0.675097\n",
      "Epoch 156 : Train loss -> 0.574536 Test loss -> 0.600168 Train acc -> 0.696778 Test acc -> 0.673991\n",
      "Epoch 157 : Train loss -> 0.574420 Test loss -> 0.600167 Train acc -> 0.697133 Test acc -> 0.674129\n",
      "Epoch 158 : Train loss -> 0.574311 Test loss -> 0.600271 Train acc -> 0.696541 Test acc -> 0.675373\n",
      "Epoch 159 : Train loss -> 0.574188 Test loss -> 0.600203 Train acc -> 0.697251 Test acc -> 0.674820\n",
      "Epoch 160 : Train loss -> 0.574073 Test loss -> 0.600288 Train acc -> 0.697133 Test acc -> 0.674406\n",
      "Epoch 161 : Train loss -> 0.573961 Test loss -> 0.600181 Train acc -> 0.696955 Test acc -> 0.674544\n",
      "Epoch 162 : Train loss -> 0.573844 Test loss -> 0.600153 Train acc -> 0.697133 Test acc -> 0.673991\n",
      "Epoch 163 : Train loss -> 0.573738 Test loss -> 0.600210 Train acc -> 0.697192 Test acc -> 0.674682\n",
      "Epoch 164 : Train loss -> 0.573615 Test loss -> 0.600228 Train acc -> 0.696896 Test acc -> 0.674406\n",
      "Epoch 165 : Train loss -> 0.573498 Test loss -> 0.600343 Train acc -> 0.697666 Test acc -> 0.674268\n",
      "Epoch 166 : Train loss -> 0.573381 Test loss -> 0.600354 Train acc -> 0.697370 Test acc -> 0.673991\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 167 : Train loss -> 0.573275 Test loss -> 0.600331 Train acc -> 0.697725 Test acc -> 0.675235\n",
      "Epoch 168 : Train loss -> 0.573154 Test loss -> 0.600343 Train acc -> 0.697725 Test acc -> 0.673991\n",
      "Epoch 169 : Train loss -> 0.573039 Test loss -> 0.600379 Train acc -> 0.697607 Test acc -> 0.674682\n",
      "Epoch 170 : Train loss -> 0.572928 Test loss -> 0.600373 Train acc -> 0.698318 Test acc -> 0.674129\n",
      "Epoch 171 : Train loss -> 0.572826 Test loss -> 0.600353 Train acc -> 0.698614 Test acc -> 0.674544\n",
      "Epoch 172 : Train loss -> 0.572715 Test loss -> 0.600405 Train acc -> 0.698614 Test acc -> 0.674682\n",
      "Epoch 173 : Train loss -> 0.572612 Test loss -> 0.600460 Train acc -> 0.698614 Test acc -> 0.674406\n",
      "Epoch 174 : Train loss -> 0.572488 Test loss -> 0.600467 Train acc -> 0.698377 Test acc -> 0.673991\n",
      "Epoch 175 : Train loss -> 0.572380 Test loss -> 0.600530 Train acc -> 0.698673 Test acc -> 0.673853\n",
      "Epoch 176 : Train loss -> 0.572271 Test loss -> 0.600523 Train acc -> 0.698732 Test acc -> 0.673853\n",
      "Epoch 177 : Train loss -> 0.572169 Test loss -> 0.600490 Train acc -> 0.698910 Test acc -> 0.674129\n",
      "Epoch 178 : Train loss -> 0.572055 Test loss -> 0.600623 Train acc -> 0.699206 Test acc -> 0.673715\n",
      "Epoch 179 : Train loss -> 0.571953 Test loss -> 0.600524 Train acc -> 0.699147 Test acc -> 0.673438\n",
      "Epoch 180 : Train loss -> 0.571837 Test loss -> 0.600623 Train acc -> 0.699325 Test acc -> 0.673438\n",
      "Epoch 181 : Train loss -> 0.571727 Test loss -> 0.600649 Train acc -> 0.699147 Test acc -> 0.673715\n",
      "Epoch 182 : Train loss -> 0.571619 Test loss -> 0.600692 Train acc -> 0.699739 Test acc -> 0.673162\n",
      "Epoch 183 : Train loss -> 0.571505 Test loss -> 0.600715 Train acc -> 0.699502 Test acc -> 0.673300\n",
      "Epoch 184 : Train loss -> 0.571402 Test loss -> 0.600817 Train acc -> 0.699739 Test acc -> 0.673162\n",
      "Epoch 185 : Train loss -> 0.571294 Test loss -> 0.600851 Train acc -> 0.699029 Test acc -> 0.673162\n",
      "Epoch 186 : Train loss -> 0.571182 Test loss -> 0.600780 Train acc -> 0.699680 Test acc -> 0.673577\n",
      "Epoch 187 : Train loss -> 0.571081 Test loss -> 0.600709 Train acc -> 0.699799 Test acc -> 0.673438\n",
      "Epoch 188 : Train loss -> 0.570972 Test loss -> 0.600915 Train acc -> 0.700332 Test acc -> 0.673577\n",
      "Epoch 189 : Train loss -> 0.570864 Test loss -> 0.600897 Train acc -> 0.700095 Test acc -> 0.672609\n",
      "Epoch 190 : Train loss -> 0.570758 Test loss -> 0.600883 Train acc -> 0.700213 Test acc -> 0.673024\n",
      "Epoch 191 : Train loss -> 0.570645 Test loss -> 0.601061 Train acc -> 0.700450 Test acc -> 0.673024\n",
      "Epoch 192 : Train loss -> 0.570538 Test loss -> 0.601002 Train acc -> 0.700391 Test acc -> 0.673300\n",
      "Epoch 193 : Train loss -> 0.570446 Test loss -> 0.601121 Train acc -> 0.700213 Test acc -> 0.673577\n",
      "Epoch 194 : Train loss -> 0.570323 Test loss -> 0.601064 Train acc -> 0.700806 Test acc -> 0.672886\n",
      "Epoch 195 : Train loss -> 0.570209 Test loss -> 0.601094 Train acc -> 0.700509 Test acc -> 0.672471\n",
      "Epoch 196 : Train loss -> 0.570103 Test loss -> 0.601126 Train acc -> 0.700569 Test acc -> 0.671918\n",
      "Epoch 197 : Train loss -> 0.569996 Test loss -> 0.601093 Train acc -> 0.700628 Test acc -> 0.672471\n",
      "Epoch 198 : Train loss -> 0.569893 Test loss -> 0.601146 Train acc -> 0.701043 Test acc -> 0.672056\n",
      "Epoch 199 : Train loss -> 0.569784 Test loss -> 0.601264 Train acc -> 0.700983 Test acc -> 0.672471\n",
      "Epoch 200 : Train loss -> 0.569681 Test loss -> 0.601209 Train acc -> 0.700983 Test acc -> 0.671780\n",
      "Epoch 201 : Train loss -> 0.569575 Test loss -> 0.601284 Train acc -> 0.701279 Test acc -> 0.672747\n",
      "Epoch 202 : Train loss -> 0.569468 Test loss -> 0.601342 Train acc -> 0.701398 Test acc -> 0.672471\n",
      "Epoch 203 : Train loss -> 0.569365 Test loss -> 0.601374 Train acc -> 0.701279 Test acc -> 0.672747\n",
      "Epoch 204 : Train loss -> 0.569261 Test loss -> 0.601383 Train acc -> 0.701398 Test acc -> 0.671918\n",
      "Epoch 205 : Train loss -> 0.569152 Test loss -> 0.601403 Train acc -> 0.701398 Test acc -> 0.671918\n",
      "Epoch 206 : Train loss -> 0.569043 Test loss -> 0.601470 Train acc -> 0.701694 Test acc -> 0.671780\n",
      "Epoch 207 : Train loss -> 0.568940 Test loss -> 0.601504 Train acc -> 0.701339 Test acc -> 0.671089\n",
      "Epoch 208 : Train loss -> 0.568830 Test loss -> 0.601564 Train acc -> 0.701635 Test acc -> 0.671918\n",
      "Epoch 209 : Train loss -> 0.568728 Test loss -> 0.601564 Train acc -> 0.701398 Test acc -> 0.671780\n",
      "Epoch 210 : Train loss -> 0.568638 Test loss -> 0.601608 Train acc -> 0.701339 Test acc -> 0.671642\n",
      "Epoch 211 : Train loss -> 0.568514 Test loss -> 0.601625 Train acc -> 0.701576 Test acc -> 0.671780\n",
      "Epoch 212 : Train loss -> 0.568410 Test loss -> 0.601622 Train acc -> 0.701398 Test acc -> 0.671780\n",
      "Epoch 213 : Train loss -> 0.568294 Test loss -> 0.601793 Train acc -> 0.701220 Test acc -> 0.672195\n",
      "Epoch 214 : Train loss -> 0.568191 Test loss -> 0.601729 Train acc -> 0.701576 Test acc -> 0.671918\n",
      "Epoch 215 : Train loss -> 0.568085 Test loss -> 0.601779 Train acc -> 0.701457 Test acc -> 0.672609\n",
      "Epoch 216 : Train loss -> 0.567981 Test loss -> 0.601832 Train acc -> 0.701161 Test acc -> 0.672609\n",
      "Epoch 217 : Train loss -> 0.567885 Test loss -> 0.601787 Train acc -> 0.701398 Test acc -> 0.672333\n",
      "Epoch 218 : Train loss -> 0.567797 Test loss -> 0.601805 Train acc -> 0.701813 Test acc -> 0.671780\n",
      "Epoch 219 : Train loss -> 0.567680 Test loss -> 0.601860 Train acc -> 0.701043 Test acc -> 0.671918\n",
      "Epoch 220 : Train loss -> 0.567593 Test loss -> 0.602014 Train acc -> 0.701161 Test acc -> 0.672195\n",
      "Epoch 221 : Train loss -> 0.567496 Test loss -> 0.602022 Train acc -> 0.701813 Test acc -> 0.671918\n",
      "Epoch 222 : Train loss -> 0.567374 Test loss -> 0.602055 Train acc -> 0.701102 Test acc -> 0.672056\n",
      "Epoch 223 : Train loss -> 0.567269 Test loss -> 0.602049 Train acc -> 0.701931 Test acc -> 0.671780\n",
      "Epoch 224 : Train loss -> 0.567174 Test loss -> 0.602110 Train acc -> 0.701635 Test acc -> 0.671918\n",
      "Epoch 225 : Train loss -> 0.567069 Test loss -> 0.602022 Train acc -> 0.701990 Test acc -> 0.671504\n",
      "Epoch 226 : Train loss -> 0.566968 Test loss -> 0.602209 Train acc -> 0.701516 Test acc -> 0.671504\n",
      "Epoch 227 : Train loss -> 0.566869 Test loss -> 0.602154 Train acc -> 0.701457 Test acc -> 0.671642\n",
      "Epoch 228 : Train loss -> 0.566767 Test loss -> 0.602181 Train acc -> 0.701931 Test acc -> 0.672195\n",
      "Epoch 229 : Train loss -> 0.566655 Test loss -> 0.602312 Train acc -> 0.701813 Test acc -> 0.671918\n",
      "Epoch 230 : Train loss -> 0.566554 Test loss -> 0.602366 Train acc -> 0.702583 Test acc -> 0.671780\n",
      "Epoch 231 : Train loss -> 0.566441 Test loss -> 0.602395 Train acc -> 0.702286 Test acc -> 0.670951\n",
      "Epoch 232 : Train loss -> 0.566355 Test loss -> 0.602396 Train acc -> 0.702583 Test acc -> 0.671918\n",
      "Epoch 233 : Train loss -> 0.566245 Test loss -> 0.602488 Train acc -> 0.702109 Test acc -> 0.671365\n",
      "Epoch 234 : Train loss -> 0.566150 Test loss -> 0.602562 Train acc -> 0.702642 Test acc -> 0.671918\n",
      "Epoch 235 : Train loss -> 0.566032 Test loss -> 0.602568 Train acc -> 0.702286 Test acc -> 0.671780\n",
      "Epoch 236 : Train loss -> 0.565932 Test loss -> 0.602645 Train acc -> 0.702642 Test acc -> 0.672056\n",
      "Epoch 237 : Train loss -> 0.565825 Test loss -> 0.602777 Train acc -> 0.702701 Test acc -> 0.671504\n",
      "Epoch 238 : Train loss -> 0.565725 Test loss -> 0.602774 Train acc -> 0.702523 Test acc -> 0.671780\n",
      "Epoch 239 : Train loss -> 0.565623 Test loss -> 0.602864 Train acc -> 0.703057 Test acc -> 0.671780\n",
      "Epoch 240 : Train loss -> 0.565524 Test loss -> 0.602906 Train acc -> 0.702405 Test acc -> 0.672333\n",
      "Epoch 241 : Train loss -> 0.565422 Test loss -> 0.602874 Train acc -> 0.703057 Test acc -> 0.672471\n",
      "Epoch 242 : Train loss -> 0.565323 Test loss -> 0.603018 Train acc -> 0.702523 Test acc -> 0.672333\n",
      "Epoch 243 : Train loss -> 0.565230 Test loss -> 0.603009 Train acc -> 0.703057 Test acc -> 0.671642\n",
      "Epoch 244 : Train loss -> 0.565126 Test loss -> 0.603002 Train acc -> 0.702523 Test acc -> 0.672056\n",
      "Epoch 245 : Train loss -> 0.565028 Test loss -> 0.603131 Train acc -> 0.702820 Test acc -> 0.671365\n",
      "Epoch 246 : Train loss -> 0.564955 Test loss -> 0.603113 Train acc -> 0.702997 Test acc -> 0.671365\n",
      "Epoch 247 : Train loss -> 0.564844 Test loss -> 0.603180 Train acc -> 0.702997 Test acc -> 0.671780\n",
      "Epoch 248 : Train loss -> 0.564752 Test loss -> 0.603299 Train acc -> 0.703175 Test acc -> 0.671504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 249 : Train loss -> 0.564656 Test loss -> 0.603099 Train acc -> 0.703293 Test acc -> 0.671227\n",
      "Epoch 250 : Train loss -> 0.564538 Test loss -> 0.603279 Train acc -> 0.703471 Test acc -> 0.671365\n",
      "Epoch 251 : Train loss -> 0.564449 Test loss -> 0.603287 Train acc -> 0.702879 Test acc -> 0.671780\n",
      "Epoch 252 : Train loss -> 0.564340 Test loss -> 0.603378 Train acc -> 0.703175 Test acc -> 0.671780\n",
      "Epoch 253 : Train loss -> 0.564249 Test loss -> 0.603340 Train acc -> 0.703293 Test acc -> 0.671918\n",
      "Epoch 254 : Train loss -> 0.564146 Test loss -> 0.603441 Train acc -> 0.703412 Test acc -> 0.671780\n",
      "Epoch 255 : Train loss -> 0.564053 Test loss -> 0.603590 Train acc -> 0.702701 Test acc -> 0.670536\n",
      "Epoch 256 : Train loss -> 0.563953 Test loss -> 0.603710 Train acc -> 0.703530 Test acc -> 0.671227\n",
      "Epoch 257 : Train loss -> 0.563886 Test loss -> 0.603644 Train acc -> 0.703353 Test acc -> 0.671365\n",
      "Epoch 258 : Train loss -> 0.563763 Test loss -> 0.603523 Train acc -> 0.703471 Test acc -> 0.670951\n",
      "Epoch 259 : Train loss -> 0.563662 Test loss -> 0.603701 Train acc -> 0.702938 Test acc -> 0.671089\n",
      "Epoch 260 : Train loss -> 0.563577 Test loss -> 0.603661 Train acc -> 0.703590 Test acc -> 0.670951\n",
      "Epoch 261 : Train loss -> 0.563481 Test loss -> 0.603787 Train acc -> 0.703412 Test acc -> 0.670674\n",
      "Epoch 262 : Train loss -> 0.563389 Test loss -> 0.603745 Train acc -> 0.703353 Test acc -> 0.670813\n",
      "Epoch 263 : Train loss -> 0.563297 Test loss -> 0.603808 Train acc -> 0.703827 Test acc -> 0.670398\n",
      "Epoch 264 : Train loss -> 0.563199 Test loss -> 0.604040 Train acc -> 0.703649 Test acc -> 0.670951\n",
      "Epoch 265 : Train loss -> 0.563107 Test loss -> 0.603894 Train acc -> 0.704063 Test acc -> 0.670674\n",
      "Epoch 266 : Train loss -> 0.563025 Test loss -> 0.603985 Train acc -> 0.703827 Test acc -> 0.670536\n",
      "Epoch 267 : Train loss -> 0.562918 Test loss -> 0.604031 Train acc -> 0.703353 Test acc -> 0.670398\n",
      "Epoch 268 : Train loss -> 0.562830 Test loss -> 0.604113 Train acc -> 0.703353 Test acc -> 0.670813\n",
      "Epoch 269 : Train loss -> 0.562737 Test loss -> 0.604199 Train acc -> 0.703412 Test acc -> 0.671227\n",
      "Epoch 270 : Train loss -> 0.562653 Test loss -> 0.604117 Train acc -> 0.704063 Test acc -> 0.669845\n",
      "Epoch 271 : Train loss -> 0.562554 Test loss -> 0.604263 Train acc -> 0.703827 Test acc -> 0.669983\n",
      "Epoch 272 : Train loss -> 0.562462 Test loss -> 0.604316 Train acc -> 0.703945 Test acc -> 0.669845\n",
      "Epoch 273 : Train loss -> 0.562377 Test loss -> 0.604325 Train acc -> 0.704478 Test acc -> 0.670122\n",
      "Epoch 274 : Train loss -> 0.562293 Test loss -> 0.604346 Train acc -> 0.703530 Test acc -> 0.670536\n",
      "Epoch 275 : Train loss -> 0.562194 Test loss -> 0.604412 Train acc -> 0.704182 Test acc -> 0.669569\n",
      "Epoch 276 : Train loss -> 0.562102 Test loss -> 0.604568 Train acc -> 0.704241 Test acc -> 0.670260\n",
      "Epoch 277 : Train loss -> 0.562014 Test loss -> 0.604524 Train acc -> 0.704893 Test acc -> 0.669983\n",
      "Epoch 278 : Train loss -> 0.561928 Test loss -> 0.604483 Train acc -> 0.704537 Test acc -> 0.670122\n",
      "Epoch 279 : Train loss -> 0.561834 Test loss -> 0.604686 Train acc -> 0.704715 Test acc -> 0.669707\n",
      "Epoch 280 : Train loss -> 0.561747 Test loss -> 0.604699 Train acc -> 0.704834 Test acc -> 0.669431\n",
      "Epoch 281 : Train loss -> 0.561658 Test loss -> 0.604834 Train acc -> 0.704597 Test acc -> 0.669845\n",
      "Epoch 282 : Train loss -> 0.561568 Test loss -> 0.604927 Train acc -> 0.704715 Test acc -> 0.669016\n",
      "Epoch 283 : Train loss -> 0.561473 Test loss -> 0.604956 Train acc -> 0.705189 Test acc -> 0.669845\n",
      "Epoch 284 : Train loss -> 0.561394 Test loss -> 0.604904 Train acc -> 0.705367 Test acc -> 0.669845\n",
      "Epoch 285 : Train loss -> 0.561304 Test loss -> 0.604977 Train acc -> 0.704834 Test acc -> 0.669292\n",
      "Epoch 286 : Train loss -> 0.561196 Test loss -> 0.605094 Train acc -> 0.705070 Test acc -> 0.668187\n",
      "Epoch 287 : Train loss -> 0.561111 Test loss -> 0.605034 Train acc -> 0.705248 Test acc -> 0.668878\n",
      "Epoch 288 : Train loss -> 0.561029 Test loss -> 0.605086 Train acc -> 0.705248 Test acc -> 0.668463\n",
      "Epoch 289 : Train loss -> 0.560930 Test loss -> 0.605070 Train acc -> 0.705367 Test acc -> 0.667910\n",
      "Epoch 290 : Train loss -> 0.560841 Test loss -> 0.605234 Train acc -> 0.705070 Test acc -> 0.668740\n",
      "Epoch 291 : Train loss -> 0.560747 Test loss -> 0.605331 Train acc -> 0.705189 Test acc -> 0.667634\n",
      "Epoch 292 : Train loss -> 0.560661 Test loss -> 0.605473 Train acc -> 0.705544 Test acc -> 0.667910\n",
      "Epoch 293 : Train loss -> 0.560571 Test loss -> 0.605457 Train acc -> 0.705307 Test acc -> 0.668463\n",
      "Epoch 294 : Train loss -> 0.560485 Test loss -> 0.605478 Train acc -> 0.705189 Test acc -> 0.668187\n",
      "Epoch 295 : Train loss -> 0.560386 Test loss -> 0.605548 Train acc -> 0.705189 Test acc -> 0.668601\n",
      "Epoch 296 : Train loss -> 0.560323 Test loss -> 0.605619 Train acc -> 0.706137 Test acc -> 0.667496\n",
      "Epoch 297 : Train loss -> 0.560215 Test loss -> 0.605664 Train acc -> 0.705189 Test acc -> 0.667634\n",
      "Epoch 298 : Train loss -> 0.560147 Test loss -> 0.605902 Train acc -> 0.706018 Test acc -> 0.667219\n",
      "Epoch 299 : Train loss -> 0.560050 Test loss -> 0.605686 Train acc -> 0.706433 Test acc -> 0.666667\n",
      "Epoch 300 : Train loss -> 0.559946 Test loss -> 0.605741 Train acc -> 0.706077 Test acc -> 0.667496\n",
      "Epoch 301 : Train loss -> 0.559861 Test loss -> 0.605799 Train acc -> 0.706196 Test acc -> 0.667219\n",
      "Epoch 302 : Train loss -> 0.559770 Test loss -> 0.605850 Train acc -> 0.705841 Test acc -> 0.666805\n",
      "Epoch 303 : Train loss -> 0.559690 Test loss -> 0.605932 Train acc -> 0.706551 Test acc -> 0.666667\n",
      "Epoch 304 : Train loss -> 0.559599 Test loss -> 0.606034 Train acc -> 0.706255 Test acc -> 0.667634\n",
      "Epoch 305 : Train loss -> 0.559507 Test loss -> 0.606173 Train acc -> 0.706433 Test acc -> 0.667634\n",
      "Epoch 306 : Train loss -> 0.559421 Test loss -> 0.606102 Train acc -> 0.706492 Test acc -> 0.667496\n",
      "Epoch 307 : Train loss -> 0.559330 Test loss -> 0.606130 Train acc -> 0.706492 Test acc -> 0.667634\n",
      "Epoch 308 : Train loss -> 0.559241 Test loss -> 0.606223 Train acc -> 0.706551 Test acc -> 0.667358\n",
      "Epoch 309 : Train loss -> 0.559162 Test loss -> 0.606385 Train acc -> 0.706433 Test acc -> 0.666114\n",
      "Epoch 310 : Train loss -> 0.559073 Test loss -> 0.606290 Train acc -> 0.707203 Test acc -> 0.666390\n",
      "Epoch 311 : Train loss -> 0.558978 Test loss -> 0.606495 Train acc -> 0.706492 Test acc -> 0.666114\n",
      "Epoch 312 : Train loss -> 0.558902 Test loss -> 0.606349 Train acc -> 0.706966 Test acc -> 0.666667\n",
      "Epoch 313 : Train loss -> 0.558814 Test loss -> 0.606495 Train acc -> 0.706729 Test acc -> 0.666667\n",
      "Epoch 314 : Train loss -> 0.558735 Test loss -> 0.606515 Train acc -> 0.707440 Test acc -> 0.665285\n",
      "Epoch 315 : Train loss -> 0.558640 Test loss -> 0.606472 Train acc -> 0.706966 Test acc -> 0.665837\n",
      "Epoch 316 : Train loss -> 0.558559 Test loss -> 0.606589 Train acc -> 0.707855 Test acc -> 0.666943\n",
      "Epoch 317 : Train loss -> 0.558465 Test loss -> 0.606578 Train acc -> 0.707677 Test acc -> 0.666667\n",
      "Epoch 318 : Train loss -> 0.558366 Test loss -> 0.606722 Train acc -> 0.707618 Test acc -> 0.666114\n",
      "Epoch 319 : Train loss -> 0.558280 Test loss -> 0.606822 Train acc -> 0.708091 Test acc -> 0.665423\n",
      "Epoch 320 : Train loss -> 0.558200 Test loss -> 0.606708 Train acc -> 0.708091 Test acc -> 0.665146\n",
      "Epoch 321 : Train loss -> 0.558104 Test loss -> 0.606771 Train acc -> 0.708091 Test acc -> 0.665285\n",
      "Epoch 322 : Train loss -> 0.558015 Test loss -> 0.606824 Train acc -> 0.708269 Test acc -> 0.664594\n",
      "Epoch 323 : Train loss -> 0.557922 Test loss -> 0.606813 Train acc -> 0.708328 Test acc -> 0.665699\n",
      "Epoch 324 : Train loss -> 0.557828 Test loss -> 0.606957 Train acc -> 0.708447 Test acc -> 0.665423\n",
      "Epoch 325 : Train loss -> 0.557737 Test loss -> 0.606917 Train acc -> 0.708328 Test acc -> 0.665837\n",
      "Epoch 326 : Train loss -> 0.557657 Test loss -> 0.607077 Train acc -> 0.708210 Test acc -> 0.665285\n",
      "Epoch 327 : Train loss -> 0.557559 Test loss -> 0.607085 Train acc -> 0.708210 Test acc -> 0.664870\n",
      "Epoch 328 : Train loss -> 0.557477 Test loss -> 0.606968 Train acc -> 0.708447 Test acc -> 0.665146\n",
      "Epoch 329 : Train loss -> 0.557394 Test loss -> 0.607222 Train acc -> 0.708743 Test acc -> 0.665699\n",
      "Epoch 330 : Train loss -> 0.557294 Test loss -> 0.607231 Train acc -> 0.709039 Test acc -> 0.665976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 331 : Train loss -> 0.557194 Test loss -> 0.607298 Train acc -> 0.709158 Test acc -> 0.665561\n",
      "Epoch 332 : Train loss -> 0.557109 Test loss -> 0.607376 Train acc -> 0.709098 Test acc -> 0.666528\n",
      "Epoch 333 : Train loss -> 0.557014 Test loss -> 0.607351 Train acc -> 0.709513 Test acc -> 0.665976\n",
      "Epoch 334 : Train loss -> 0.556929 Test loss -> 0.607413 Train acc -> 0.709513 Test acc -> 0.666114\n",
      "Epoch 335 : Train loss -> 0.556848 Test loss -> 0.607511 Train acc -> 0.709098 Test acc -> 0.666252\n",
      "Epoch 336 : Train loss -> 0.556742 Test loss -> 0.607511 Train acc -> 0.709513 Test acc -> 0.666114\n",
      "Epoch 337 : Train loss -> 0.556649 Test loss -> 0.607627 Train acc -> 0.709750 Test acc -> 0.665561\n",
      "Epoch 338 : Train loss -> 0.556593 Test loss -> 0.607721 Train acc -> 0.709632 Test acc -> 0.665699\n",
      "Epoch 339 : Train loss -> 0.556477 Test loss -> 0.607592 Train acc -> 0.709928 Test acc -> 0.665976\n",
      "Epoch 340 : Train loss -> 0.556390 Test loss -> 0.607803 Train acc -> 0.710283 Test acc -> 0.665285\n",
      "Epoch 341 : Train loss -> 0.556318 Test loss -> 0.607790 Train acc -> 0.710165 Test acc -> 0.665146\n",
      "Epoch 342 : Train loss -> 0.556207 Test loss -> 0.607701 Train acc -> 0.709868 Test acc -> 0.666114\n",
      "Epoch 343 : Train loss -> 0.556110 Test loss -> 0.607833 Train acc -> 0.710224 Test acc -> 0.666252\n",
      "Epoch 344 : Train loss -> 0.556016 Test loss -> 0.607908 Train acc -> 0.710579 Test acc -> 0.666252\n",
      "Epoch 345 : Train loss -> 0.555935 Test loss -> 0.607922 Train acc -> 0.710402 Test acc -> 0.665699\n",
      "Epoch 346 : Train loss -> 0.555831 Test loss -> 0.608142 Train acc -> 0.710579 Test acc -> 0.665699\n",
      "Epoch 347 : Train loss -> 0.555738 Test loss -> 0.608166 Train acc -> 0.710698 Test acc -> 0.665976\n",
      "Epoch 348 : Train loss -> 0.555648 Test loss -> 0.608170 Train acc -> 0.711112 Test acc -> 0.665699\n",
      "Epoch 349 : Train loss -> 0.555554 Test loss -> 0.608268 Train acc -> 0.710994 Test acc -> 0.665699\n",
      "Epoch 350 : Train loss -> 0.555471 Test loss -> 0.608169 Train acc -> 0.710816 Test acc -> 0.665285\n",
      "Epoch 351 : Train loss -> 0.555369 Test loss -> 0.608378 Train acc -> 0.711705 Test acc -> 0.664732\n",
      "Epoch 352 : Train loss -> 0.555291 Test loss -> 0.608374 Train acc -> 0.711231 Test acc -> 0.665561\n",
      "Epoch 353 : Train loss -> 0.555194 Test loss -> 0.608396 Train acc -> 0.711586 Test acc -> 0.664732\n",
      "Epoch 354 : Train loss -> 0.555104 Test loss -> 0.608411 Train acc -> 0.710579 Test acc -> 0.664870\n",
      "Epoch 355 : Train loss -> 0.555000 Test loss -> 0.608572 Train acc -> 0.711053 Test acc -> 0.665285\n",
      "Epoch 356 : Train loss -> 0.554909 Test loss -> 0.608750 Train acc -> 0.711053 Test acc -> 0.665285\n",
      "Epoch 357 : Train loss -> 0.554815 Test loss -> 0.608738 Train acc -> 0.711764 Test acc -> 0.664732\n",
      "Epoch 358 : Train loss -> 0.554734 Test loss -> 0.608825 Train acc -> 0.711231 Test acc -> 0.665976\n",
      "Epoch 359 : Train loss -> 0.554628 Test loss -> 0.608813 Train acc -> 0.710994 Test acc -> 0.665561\n",
      "Epoch 360 : Train loss -> 0.554535 Test loss -> 0.608804 Train acc -> 0.711290 Test acc -> 0.664317\n",
      "Epoch 361 : Train loss -> 0.554444 Test loss -> 0.608848 Train acc -> 0.712060 Test acc -> 0.664870\n",
      "Epoch 362 : Train loss -> 0.554348 Test loss -> 0.609120 Train acc -> 0.711764 Test acc -> 0.665008\n",
      "Epoch 363 : Train loss -> 0.554260 Test loss -> 0.609030 Train acc -> 0.711646 Test acc -> 0.665146\n",
      "Epoch 364 : Train loss -> 0.554192 Test loss -> 0.609050 Train acc -> 0.711586 Test acc -> 0.664594\n",
      "Epoch 365 : Train loss -> 0.554069 Test loss -> 0.609145 Train acc -> 0.711764 Test acc -> 0.665008\n",
      "Epoch 366 : Train loss -> 0.553986 Test loss -> 0.609193 Train acc -> 0.711409 Test acc -> 0.665146\n",
      "Epoch 367 : Train loss -> 0.553888 Test loss -> 0.609271 Train acc -> 0.711468 Test acc -> 0.664594\n",
      "Epoch 368 : Train loss -> 0.553817 Test loss -> 0.609310 Train acc -> 0.711349 Test acc -> 0.664870\n",
      "Epoch 369 : Train loss -> 0.553709 Test loss -> 0.609275 Train acc -> 0.711409 Test acc -> 0.664870\n",
      "Epoch 370 : Train loss -> 0.553608 Test loss -> 0.609457 Train acc -> 0.711942 Test acc -> 0.664732\n",
      "Epoch 371 : Train loss -> 0.553542 Test loss -> 0.609547 Train acc -> 0.711882 Test acc -> 0.664594\n",
      "Epoch 372 : Train loss -> 0.553436 Test loss -> 0.609559 Train acc -> 0.712001 Test acc -> 0.664179\n",
      "Epoch 373 : Train loss -> 0.553369 Test loss -> 0.609663 Train acc -> 0.712001 Test acc -> 0.664594\n",
      "Epoch 374 : Train loss -> 0.553255 Test loss -> 0.609952 Train acc -> 0.712297 Test acc -> 0.664179\n",
      "Epoch 375 : Train loss -> 0.553186 Test loss -> 0.610042 Train acc -> 0.712001 Test acc -> 0.663903\n",
      "Epoch 376 : Train loss -> 0.553071 Test loss -> 0.609918 Train acc -> 0.712771 Test acc -> 0.663903\n",
      "Epoch 377 : Train loss -> 0.552998 Test loss -> 0.610018 Train acc -> 0.712416 Test acc -> 0.664179\n",
      "Epoch 378 : Train loss -> 0.552885 Test loss -> 0.610043 Train acc -> 0.712830 Test acc -> 0.663626\n",
      "Epoch 379 : Train loss -> 0.552794 Test loss -> 0.610067 Train acc -> 0.712771 Test acc -> 0.663903\n",
      "Epoch 380 : Train loss -> 0.552709 Test loss -> 0.610267 Train acc -> 0.712712 Test acc -> 0.664179\n",
      "Epoch 381 : Train loss -> 0.552609 Test loss -> 0.610374 Train acc -> 0.713008 Test acc -> 0.664455\n",
      "Epoch 382 : Train loss -> 0.552538 Test loss -> 0.610366 Train acc -> 0.712771 Test acc -> 0.664732\n",
      "Epoch 383 : Train loss -> 0.552434 Test loss -> 0.610336 Train acc -> 0.713067 Test acc -> 0.664317\n",
      "Epoch 384 : Train loss -> 0.552333 Test loss -> 0.610532 Train acc -> 0.713126 Test acc -> 0.665146\n",
      "Epoch 385 : Train loss -> 0.552245 Test loss -> 0.610561 Train acc -> 0.713423 Test acc -> 0.663903\n",
      "Epoch 386 : Train loss -> 0.552156 Test loss -> 0.610716 Train acc -> 0.713363 Test acc -> 0.664594\n",
      "Epoch 387 : Train loss -> 0.552057 Test loss -> 0.610676 Train acc -> 0.713245 Test acc -> 0.664317\n",
      "Epoch 388 : Train loss -> 0.551986 Test loss -> 0.610727 Train acc -> 0.713186 Test acc -> 0.664179\n",
      "Epoch 389 : Train loss -> 0.551883 Test loss -> 0.611038 Train acc -> 0.713482 Test acc -> 0.664179\n",
      "Epoch 390 : Train loss -> 0.551873 Test loss -> 0.611060 Train acc -> 0.713600 Test acc -> 0.665008\n",
      "Epoch 391 : Train loss -> 0.551686 Test loss -> 0.611054 Train acc -> 0.713186 Test acc -> 0.664455\n",
      "Epoch 392 : Train loss -> 0.551591 Test loss -> 0.611140 Train acc -> 0.713363 Test acc -> 0.664179\n",
      "Epoch 393 : Train loss -> 0.551509 Test loss -> 0.611132 Train acc -> 0.713600 Test acc -> 0.664179\n",
      "Epoch 394 : Train loss -> 0.551411 Test loss -> 0.611267 Train acc -> 0.713186 Test acc -> 0.663765\n",
      "Epoch 395 : Train loss -> 0.551327 Test loss -> 0.611194 Train acc -> 0.713363 Test acc -> 0.663903\n",
      "Epoch 396 : Train loss -> 0.551233 Test loss -> 0.611273 Train acc -> 0.713541 Test acc -> 0.664179\n",
      "Epoch 397 : Train loss -> 0.551140 Test loss -> 0.611379 Train acc -> 0.713956 Test acc -> 0.663488\n",
      "Epoch 398 : Train loss -> 0.551085 Test loss -> 0.611623 Train acc -> 0.713363 Test acc -> 0.665699\n",
      "Epoch 399 : Train loss -> 0.550955 Test loss -> 0.611563 Train acc -> 0.713304 Test acc -> 0.664179\n",
      "Epoch 400 : Train loss -> 0.550867 Test loss -> 0.611758 Train acc -> 0.713660 Test acc -> 0.663903\n",
      "Epoch 401 : Train loss -> 0.550780 Test loss -> 0.611592 Train acc -> 0.713837 Test acc -> 0.664594\n",
      "Epoch 402 : Train loss -> 0.550695 Test loss -> 0.611704 Train acc -> 0.713837 Test acc -> 0.663626\n",
      "Epoch 403 : Train loss -> 0.550611 Test loss -> 0.611963 Train acc -> 0.714430 Test acc -> 0.663626\n",
      "Epoch 404 : Train loss -> 0.550504 Test loss -> 0.611724 Train acc -> 0.713719 Test acc -> 0.665423\n",
      "Epoch 405 : Train loss -> 0.550410 Test loss -> 0.611894 Train acc -> 0.714015 Test acc -> 0.665008\n",
      "Epoch 406 : Train loss -> 0.550323 Test loss -> 0.612026 Train acc -> 0.714430 Test acc -> 0.664317\n",
      "Epoch 407 : Train loss -> 0.550231 Test loss -> 0.612149 Train acc -> 0.714133 Test acc -> 0.664455\n",
      "Epoch 408 : Train loss -> 0.550134 Test loss -> 0.612174 Train acc -> 0.714133 Test acc -> 0.664317\n",
      "Epoch 409 : Train loss -> 0.550040 Test loss -> 0.612199 Train acc -> 0.714015 Test acc -> 0.664732\n",
      "Epoch 410 : Train loss -> 0.549953 Test loss -> 0.612268 Train acc -> 0.714430 Test acc -> 0.664455\n",
      "Epoch 411 : Train loss -> 0.549885 Test loss -> 0.612441 Train acc -> 0.714726 Test acc -> 0.664041\n",
      "Epoch 412 : Train loss -> 0.549793 Test loss -> 0.612451 Train acc -> 0.714548 Test acc -> 0.663765\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 413 : Train loss -> 0.549697 Test loss -> 0.612577 Train acc -> 0.714666 Test acc -> 0.665008\n",
      "Epoch 414 : Train loss -> 0.549678 Test loss -> 0.612466 Train acc -> 0.714785 Test acc -> 0.663074\n",
      "Epoch 415 : Train loss -> 0.549515 Test loss -> 0.612643 Train acc -> 0.714903 Test acc -> 0.664179\n",
      "Epoch 416 : Train loss -> 0.549440 Test loss -> 0.612695 Train acc -> 0.715081 Test acc -> 0.663350\n",
      "Epoch 417 : Train loss -> 0.549344 Test loss -> 0.612896 Train acc -> 0.715733 Test acc -> 0.663350\n",
      "Epoch 418 : Train loss -> 0.549250 Test loss -> 0.612837 Train acc -> 0.715970 Test acc -> 0.663488\n",
      "Epoch 419 : Train loss -> 0.549160 Test loss -> 0.613051 Train acc -> 0.715496 Test acc -> 0.663350\n",
      "Epoch 420 : Train loss -> 0.549072 Test loss -> 0.612945 Train acc -> 0.715851 Test acc -> 0.662244\n",
      "Epoch 421 : Train loss -> 0.548987 Test loss -> 0.612978 Train acc -> 0.716207 Test acc -> 0.662797\n",
      "Epoch 422 : Train loss -> 0.548923 Test loss -> 0.613009 Train acc -> 0.715081 Test acc -> 0.663074\n",
      "Epoch 423 : Train loss -> 0.548839 Test loss -> 0.613052 Train acc -> 0.715792 Test acc -> 0.662935\n",
      "Epoch 424 : Train loss -> 0.548726 Test loss -> 0.613239 Train acc -> 0.715970 Test acc -> 0.662106\n",
      "Epoch 425 : Train loss -> 0.548649 Test loss -> 0.613441 Train acc -> 0.716147 Test acc -> 0.662383\n",
      "Epoch 426 : Train loss -> 0.548554 Test loss -> 0.613464 Train acc -> 0.716029 Test acc -> 0.661553\n",
      "Epoch 427 : Train loss -> 0.548458 Test loss -> 0.613538 Train acc -> 0.715970 Test acc -> 0.662521\n",
      "Epoch 428 : Train loss -> 0.548400 Test loss -> 0.613611 Train acc -> 0.716266 Test acc -> 0.661830\n",
      "Epoch 429 : Train loss -> 0.548286 Test loss -> 0.613790 Train acc -> 0.716384 Test acc -> 0.662935\n",
      "Epoch 430 : Train loss -> 0.548218 Test loss -> 0.613744 Train acc -> 0.716029 Test acc -> 0.661553\n",
      "Epoch 431 : Train loss -> 0.548124 Test loss -> 0.613705 Train acc -> 0.716384 Test acc -> 0.661139\n",
      "Epoch 432 : Train loss -> 0.548022 Test loss -> 0.613984 Train acc -> 0.716562 Test acc -> 0.662244\n",
      "Epoch 433 : Train loss -> 0.547946 Test loss -> 0.613992 Train acc -> 0.716088 Test acc -> 0.661968\n",
      "Epoch 434 : Train loss -> 0.547858 Test loss -> 0.614126 Train acc -> 0.717154 Test acc -> 0.660310\n",
      "Epoch 435 : Train loss -> 0.547772 Test loss -> 0.614186 Train acc -> 0.716621 Test acc -> 0.661139\n",
      "Epoch 436 : Train loss -> 0.547709 Test loss -> 0.614193 Train acc -> 0.716147 Test acc -> 0.660862\n",
      "Epoch 437 : Train loss -> 0.547601 Test loss -> 0.614249 Train acc -> 0.717036 Test acc -> 0.661139\n",
      "Epoch 438 : Train loss -> 0.547525 Test loss -> 0.614205 Train acc -> 0.716621 Test acc -> 0.660171\n",
      "Epoch 439 : Train loss -> 0.547438 Test loss -> 0.614618 Train acc -> 0.717036 Test acc -> 0.661415\n",
      "Epoch 440 : Train loss -> 0.547345 Test loss -> 0.614597 Train acc -> 0.717214 Test acc -> 0.661277\n",
      "Epoch 441 : Train loss -> 0.547269 Test loss -> 0.614743 Train acc -> 0.717214 Test acc -> 0.660448\n",
      "Epoch 442 : Train loss -> 0.547182 Test loss -> 0.614613 Train acc -> 0.716977 Test acc -> 0.661692\n",
      "Epoch 443 : Train loss -> 0.547114 Test loss -> 0.614785 Train acc -> 0.717924 Test acc -> 0.660171\n",
      "Epoch 444 : Train loss -> 0.547013 Test loss -> 0.614691 Train acc -> 0.717332 Test acc -> 0.660862\n",
      "Epoch 445 : Train loss -> 0.546943 Test loss -> 0.614827 Train acc -> 0.717036 Test acc -> 0.660171\n",
      "Epoch 446 : Train loss -> 0.546856 Test loss -> 0.614893 Train acc -> 0.718102 Test acc -> 0.660724\n",
      "Epoch 447 : Train loss -> 0.546816 Test loss -> 0.615081 Train acc -> 0.717747 Test acc -> 0.660724\n",
      "Epoch 448 : Train loss -> 0.546688 Test loss -> 0.615179 Train acc -> 0.717569 Test acc -> 0.660448\n",
      "Epoch 449 : Train loss -> 0.546605 Test loss -> 0.615269 Train acc -> 0.717865 Test acc -> 0.660171\n",
      "Epoch 450 : Train loss -> 0.546523 Test loss -> 0.615373 Train acc -> 0.717687 Test acc -> 0.659895\n",
      "Epoch 451 : Train loss -> 0.546445 Test loss -> 0.615338 Train acc -> 0.717806 Test acc -> 0.660171\n",
      "Epoch 452 : Train loss -> 0.546365 Test loss -> 0.615508 Train acc -> 0.718458 Test acc -> 0.660033\n",
      "Epoch 453 : Train loss -> 0.546285 Test loss -> 0.615374 Train acc -> 0.718872 Test acc -> 0.659895\n",
      "Epoch 454 : Train loss -> 0.546201 Test loss -> 0.615409 Train acc -> 0.718576 Test acc -> 0.660171\n",
      "Epoch 455 : Train loss -> 0.546122 Test loss -> 0.615759 Train acc -> 0.718458 Test acc -> 0.659342\n",
      "Epoch 456 : Train loss -> 0.546038 Test loss -> 0.615720 Train acc -> 0.718813 Test acc -> 0.659342\n",
      "Epoch 457 : Train loss -> 0.545959 Test loss -> 0.615691 Train acc -> 0.718458 Test acc -> 0.659757\n",
      "Epoch 458 : Train loss -> 0.545883 Test loss -> 0.615740 Train acc -> 0.719405 Test acc -> 0.659204\n",
      "Epoch 459 : Train loss -> 0.545799 Test loss -> 0.615611 Train acc -> 0.719346 Test acc -> 0.658651\n",
      "Epoch 460 : Train loss -> 0.545725 Test loss -> 0.615760 Train acc -> 0.718754 Test acc -> 0.659480\n",
      "Epoch 461 : Train loss -> 0.545641 Test loss -> 0.615897 Train acc -> 0.719109 Test acc -> 0.658789\n",
      "Epoch 462 : Train loss -> 0.545571 Test loss -> 0.615757 Train acc -> 0.720116 Test acc -> 0.659066\n",
      "Epoch 463 : Train loss -> 0.545492 Test loss -> 0.615887 Train acc -> 0.718754 Test acc -> 0.658928\n",
      "Epoch 464 : Train loss -> 0.545410 Test loss -> 0.615934 Train acc -> 0.720827 Test acc -> 0.659619\n",
      "Epoch 465 : Train loss -> 0.545318 Test loss -> 0.616013 Train acc -> 0.720235 Test acc -> 0.659204\n",
      "Epoch 466 : Train loss -> 0.545228 Test loss -> 0.616224 Train acc -> 0.719228 Test acc -> 0.659895\n",
      "Epoch 467 : Train loss -> 0.545161 Test loss -> 0.616081 Train acc -> 0.719405 Test acc -> 0.659342\n",
      "Epoch 468 : Train loss -> 0.545082 Test loss -> 0.616336 Train acc -> 0.719701 Test acc -> 0.658375\n",
      "Epoch 469 : Train loss -> 0.545005 Test loss -> 0.616139 Train acc -> 0.719642 Test acc -> 0.658928\n",
      "Epoch 470 : Train loss -> 0.544975 Test loss -> 0.616573 Train acc -> 0.721005 Test acc -> 0.659480\n",
      "Epoch 471 : Train loss -> 0.544839 Test loss -> 0.616426 Train acc -> 0.719701 Test acc -> 0.659619\n",
      "Epoch 472 : Train loss -> 0.544758 Test loss -> 0.616309 Train acc -> 0.719820 Test acc -> 0.660171\n",
      "Epoch 473 : Train loss -> 0.544667 Test loss -> 0.616437 Train acc -> 0.720294 Test acc -> 0.659342\n",
      "Epoch 474 : Train loss -> 0.544631 Test loss -> 0.616525 Train acc -> 0.719938 Test acc -> 0.659757\n",
      "Epoch 475 : Train loss -> 0.544511 Test loss -> 0.616469 Train acc -> 0.720590 Test acc -> 0.660033\n",
      "Epoch 476 : Train loss -> 0.544421 Test loss -> 0.616543 Train acc -> 0.720235 Test acc -> 0.659342\n",
      "Epoch 477 : Train loss -> 0.544341 Test loss -> 0.616507 Train acc -> 0.720708 Test acc -> 0.659204\n",
      "Epoch 478 : Train loss -> 0.544262 Test loss -> 0.616732 Train acc -> 0.720235 Test acc -> 0.658651\n",
      "Epoch 479 : Train loss -> 0.544226 Test loss -> 0.616820 Train acc -> 0.719998 Test acc -> 0.659066\n",
      "Epoch 480 : Train loss -> 0.544102 Test loss -> 0.617057 Train acc -> 0.720472 Test acc -> 0.658928\n",
      "Epoch 481 : Train loss -> 0.544026 Test loss -> 0.617113 Train acc -> 0.720945 Test acc -> 0.659342\n",
      "Epoch 482 : Train loss -> 0.543927 Test loss -> 0.616902 Train acc -> 0.720057 Test acc -> 0.658789\n",
      "Epoch 483 : Train loss -> 0.543860 Test loss -> 0.617106 Train acc -> 0.721005 Test acc -> 0.658651\n",
      "Epoch 484 : Train loss -> 0.543786 Test loss -> 0.617249 Train acc -> 0.721419 Test acc -> 0.659204\n",
      "Epoch 485 : Train loss -> 0.543676 Test loss -> 0.617062 Train acc -> 0.721064 Test acc -> 0.658651\n",
      "Epoch 486 : Train loss -> 0.543594 Test loss -> 0.617144 Train acc -> 0.720412 Test acc -> 0.658928\n",
      "Epoch 487 : Train loss -> 0.543513 Test loss -> 0.617358 Train acc -> 0.720235 Test acc -> 0.659066\n",
      "Epoch 488 : Train loss -> 0.543422 Test loss -> 0.617361 Train acc -> 0.720294 Test acc -> 0.658513\n",
      "Epoch 489 : Train loss -> 0.543342 Test loss -> 0.617296 Train acc -> 0.721064 Test acc -> 0.659480\n",
      "Epoch 490 : Train loss -> 0.543260 Test loss -> 0.617370 Train acc -> 0.721123 Test acc -> 0.658789\n",
      "Epoch 491 : Train loss -> 0.543178 Test loss -> 0.617163 Train acc -> 0.721005 Test acc -> 0.658651\n",
      "Epoch 492 : Train loss -> 0.543094 Test loss -> 0.617490 Train acc -> 0.721479 Test acc -> 0.658789\n",
      "Epoch 493 : Train loss -> 0.542997 Test loss -> 0.617488 Train acc -> 0.721242 Test acc -> 0.659204\n",
      "Epoch 494 : Train loss -> 0.542902 Test loss -> 0.617682 Train acc -> 0.721064 Test acc -> 0.659895\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 495 : Train loss -> 0.542819 Test loss -> 0.617559 Train acc -> 0.720590 Test acc -> 0.659204\n",
      "Epoch 496 : Train loss -> 0.542745 Test loss -> 0.617577 Train acc -> 0.721182 Test acc -> 0.658928\n",
      "Epoch 497 : Train loss -> 0.542649 Test loss -> 0.617765 Train acc -> 0.721301 Test acc -> 0.658928\n",
      "Epoch 498 : Train loss -> 0.542559 Test loss -> 0.617752 Train acc -> 0.721360 Test acc -> 0.658789\n",
      "Epoch 499 : Train loss -> 0.542467 Test loss -> 0.617781 Train acc -> 0.721715 Test acc -> 0.658375\n",
      "Epoch 500 : Train loss -> 0.542394 Test loss -> 0.617738 Train acc -> 0.721893 Test acc -> 0.658928\n",
      "Epoch 501 : Train loss -> 0.542296 Test loss -> 0.617811 Train acc -> 0.721242 Test acc -> 0.658651\n",
      "Epoch 502 : Train loss -> 0.542217 Test loss -> 0.617914 Train acc -> 0.721952 Test acc -> 0.657269\n",
      "Epoch 503 : Train loss -> 0.542160 Test loss -> 0.618176 Train acc -> 0.722367 Test acc -> 0.657960\n",
      "Epoch 504 : Train loss -> 0.542032 Test loss -> 0.618090 Train acc -> 0.721893 Test acc -> 0.658789\n",
      "Epoch 505 : Train loss -> 0.541947 Test loss -> 0.617994 Train acc -> 0.722249 Test acc -> 0.658237\n",
      "Epoch 506 : Train loss -> 0.541862 Test loss -> 0.618034 Train acc -> 0.722249 Test acc -> 0.657407\n",
      "Epoch 507 : Train loss -> 0.541773 Test loss -> 0.618272 Train acc -> 0.721715 Test acc -> 0.658375\n",
      "Epoch 508 : Train loss -> 0.541685 Test loss -> 0.618288 Train acc -> 0.721952 Test acc -> 0.658928\n",
      "Epoch 509 : Train loss -> 0.541587 Test loss -> 0.618308 Train acc -> 0.722308 Test acc -> 0.658098\n",
      "Epoch 510 : Train loss -> 0.541503 Test loss -> 0.618533 Train acc -> 0.721834 Test acc -> 0.658237\n",
      "Epoch 511 : Train loss -> 0.541423 Test loss -> 0.618341 Train acc -> 0.722249 Test acc -> 0.657822\n",
      "Epoch 512 : Train loss -> 0.541369 Test loss -> 0.618561 Train acc -> 0.723256 Test acc -> 0.657684\n",
      "Epoch 513 : Train loss -> 0.541255 Test loss -> 0.618426 Train acc -> 0.721715 Test acc -> 0.658928\n",
      "Epoch 514 : Train loss -> 0.541162 Test loss -> 0.618552 Train acc -> 0.722189 Test acc -> 0.658098\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-f519a99d3d4f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/nba_prediction/dev1/trainers.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, epochs, batch_size, con, ctx)\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m                 \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlossv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m             \u001b[0mptrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0mptest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/mxnet/ndarray/ndarray.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, out_grad, retain_graph, train_mode)\u001b[0m\n\u001b[1;32m   2000\u001b[0m             \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2001\u001b[0m             \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2002\u001b[0;31m             ctypes.c_void_p(0)))\n\u001b[0m\u001b[1;32m   2003\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2004\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtostype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train(1000,256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainer.dataloader = DataLoader(trainer.train_data, trainer.train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_num = int(len(n_data)*0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_label = a[:train_num,:], r[:train_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data, test_label = a[train_num:,:], r[train_num:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((16882, 34), (7236, 34))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7236,), (16882,))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_label.shape, train_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
